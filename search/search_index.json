{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ILLIXR Illinois Extended Reality testbed or ILLIXR (pronounced like elixir) is the first fully open-source Extended Reality (XR) system and testbed. The modular, extensible, and OpenXR -compatible ILLIXR runtime integrates state-of-the-art XR components into a complete XR system. The testbed is part of the broader ILLIXR consortium , an industry-supported community effort to democratize XR systems research, development, and benchmarking. You can find the complete ILLIXR system here . ILLIXR also provides its components in standalone configurations to enable architects and system designers to research each component in isolation. The standalone components are packaged together in the v1-latest release of ILLIXR. ILLIXR's modular and extensible runtime allows adding new components and swapping different implementations of a given component. ILLIXR currently contains the following components: Perception Eye Tracking RITNet ** Scene Reconstruction ElasticFusion ** KinectFusion ** Simultaneous Localization and Mapping OpenVINS ** Kimera-VIO ** Cameras and IMUs ZED Mini Intel RealSense Visual Chromatic aberration correction Computational holography for adaptive multi-focal displays ** Lens distortion correction Asynchronous Reprojection (TimeWarp) Aural Audio encoding ** Audio playback ** (** Source is hosted in an external repository under the ILLIXR project .) We continue to add more components (new components and new implementations). Many of the current components of ILLIXR were developed by domain experts and obtained from publicly available repositories. They were modified for one or more of the following reasons: fixing compilation, adding features, or removing extraneous code or dependencies. Each component not developed by us is available as a forked github repository for proper attribution to its authors. Papers, talks, demos, consortium A paper with details on ILLIXR, including its components, runtime, telemetry support, and a comprehensive analysis of performance, power, and quality on desktop and embedded systems. A talk presented at NVIDIA GTC'21 describing ILLIXR and announcing the ILLIXR consortium: Video . Slides . A demo of an OpenXR application running with ILLIXR. The ILLIXR consortium is an industry-supported community effort to democratize XR systems research, development, and benchmarking. Visit our web site for more information. Citation We request that you cite our following paper (new version coming soon) when you use ILLIXR for a publication. We would also appreciate it if you send us a citation once your work has been published. @misc{HuzaifaDesai2020, title={Exploring Extended Reality with ILLIXR: A new Playground for Architecture Research}, author={Muhammad Huzaifa and Rishi Desai and Samuel Grayson and Xutao Jiang and Ying Jing and Jae Lee and Fang Lu and Yihan Pang and Joseph Ravichandran and Finn Sinclair and Boyuan Tian and Hengzhi Yuan and Jeffrey Zhang and Sarita V. Adve}, year={2021}, eprint={2004.04643}, primaryClass={cs.DC} } Getting Started and Documentation For more information, see our Getting Started page . Acknowledgements The ILLIXR project started in Sarita Adve\u2019s research group , co-led by PhD candidate Muhammad Huzaifa, at the University of Illinois at Urbana-Champaign. Other major contributors include Rishi Desai, Samuel Grayson, Xutao Jiang, Ying Jing, Jae Lee, Fang Lu, Yihan Pang, Joseph Ravichandran, Giordano Salvador, Finn Sinclair, Boyuan Tian, Henghzhi Yuan, and Jeffrey Zhang. ILLIXR came together after many consultations with researchers and practitioners in many domains: audio, graphics, optics, robotics, signal processing, and extended reality systems. We are deeply grateful for all of these discussions and specifically to the following: Wei Cu, Aleksandra Faust, Liang Gao, Matt Horsnell, Amit Jindal, Steve LaValle, Steve Lovegrove, Andrew Maimone, Vegard \u00d8ye, Martin Persson, Archontis Politis, Eric Shaffer, Paris Smaragdis, Sachin Talathi, and Chris Widdowson. Our OpenXR implementation is derived from Monado . We are particularly thankful to Jakob Bornecrantz and Ryan Pavlik. The development of ILLIXR was supported by the Applications Driving Architectures (ADA) Research Center (a JUMP Center co-sponsored by SRC and DARPA), the Center for Future Architectures Research (C-FAR, a STARnet research center), a Semiconductor Research Corporation program sponsored by MARCO and DARPA, and by a Google Faculty Research Award. The development of ILLIXR was also aided by generous hardware and software donations from ARM and NVIDIA. Facebook Reality Labs provided the OpenEDS Semantic Segmentation Dataset . Wesley Darvin came up with the name for ILLIXR. Licensing Structure ILLIXR is available as open-source software under the permissive University of Illinois/NCSA Open Source License . As mentioned above, ILLIXR largely consists of components developed by domain experts and modified for the purposes of inclusion in ILLIXR. However, ILLIXR does contain software developed solely by us. The NCSA license is limited to only this software . The external libraries and softwares included in ILLIXR each have their own licenses and must be used according to those licenses: ElasticFusion \\ ElasticFusion license KinectFusion \\ MIT License GTSAM \\ Simplified BSD License HOTlab \\ GNU Lesser General Public License v3.0 Kimera-VIO \\ Simplified BSD License libspatialaudio \\ GNU Lesser General Public License v2.1 Monado \\ Boost Software License 1.0 moodycamel::ConcurrentQueue \\ Simplified BSD License Open-VINS \\ GNU General Public License v3.0 RITnet \\ MIT License Note that ILLIXR's extensibility allows the source to be configured and compiled using only permissively licensed software. Get in Touch Whether you are a computer architect, a compiler writer, a systems person, work on XR related algorithms or applications, or just anyone interested in XR research, development, or products, we would love to hear from you and hope you will contribute! You can join the ILLIXR consortium , Discord , or mailing list , or send us an email , or just send us a pull request!","title":"ILLIXR (Version 2)"},{"location":"#illixr","text":"Illinois Extended Reality testbed or ILLIXR (pronounced like elixir) is the first fully open-source Extended Reality (XR) system and testbed. The modular, extensible, and OpenXR -compatible ILLIXR runtime integrates state-of-the-art XR components into a complete XR system. The testbed is part of the broader ILLIXR consortium , an industry-supported community effort to democratize XR systems research, development, and benchmarking. You can find the complete ILLIXR system here . ILLIXR also provides its components in standalone configurations to enable architects and system designers to research each component in isolation. The standalone components are packaged together in the v1-latest release of ILLIXR. ILLIXR's modular and extensible runtime allows adding new components and swapping different implementations of a given component. ILLIXR currently contains the following components: Perception Eye Tracking RITNet ** Scene Reconstruction ElasticFusion ** KinectFusion ** Simultaneous Localization and Mapping OpenVINS ** Kimera-VIO ** Cameras and IMUs ZED Mini Intel RealSense Visual Chromatic aberration correction Computational holography for adaptive multi-focal displays ** Lens distortion correction Asynchronous Reprojection (TimeWarp) Aural Audio encoding ** Audio playback ** (** Source is hosted in an external repository under the ILLIXR project .) We continue to add more components (new components and new implementations). Many of the current components of ILLIXR were developed by domain experts and obtained from publicly available repositories. They were modified for one or more of the following reasons: fixing compilation, adding features, or removing extraneous code or dependencies. Each component not developed by us is available as a forked github repository for proper attribution to its authors.","title":"ILLIXR"},{"location":"#papers-talks-demos-consortium","text":"A paper with details on ILLIXR, including its components, runtime, telemetry support, and a comprehensive analysis of performance, power, and quality on desktop and embedded systems. A talk presented at NVIDIA GTC'21 describing ILLIXR and announcing the ILLIXR consortium: Video . Slides . A demo of an OpenXR application running with ILLIXR. The ILLIXR consortium is an industry-supported community effort to democratize XR systems research, development, and benchmarking. Visit our web site for more information.","title":"Papers, talks, demos, consortium"},{"location":"#citation","text":"We request that you cite our following paper (new version coming soon) when you use ILLIXR for a publication. We would also appreciate it if you send us a citation once your work has been published. @misc{HuzaifaDesai2020, title={Exploring Extended Reality with ILLIXR: A new Playground for Architecture Research}, author={Muhammad Huzaifa and Rishi Desai and Samuel Grayson and Xutao Jiang and Ying Jing and Jae Lee and Fang Lu and Yihan Pang and Joseph Ravichandran and Finn Sinclair and Boyuan Tian and Hengzhi Yuan and Jeffrey Zhang and Sarita V. Adve}, year={2021}, eprint={2004.04643}, primaryClass={cs.DC} }","title":"Citation"},{"location":"#getting-started-and-documentation","text":"For more information, see our Getting Started page .","title":"Getting Started and Documentation"},{"location":"#acknowledgements","text":"The ILLIXR project started in Sarita Adve\u2019s research group , co-led by PhD candidate Muhammad Huzaifa, at the University of Illinois at Urbana-Champaign. Other major contributors include Rishi Desai, Samuel Grayson, Xutao Jiang, Ying Jing, Jae Lee, Fang Lu, Yihan Pang, Joseph Ravichandran, Giordano Salvador, Finn Sinclair, Boyuan Tian, Henghzhi Yuan, and Jeffrey Zhang. ILLIXR came together after many consultations with researchers and practitioners in many domains: audio, graphics, optics, robotics, signal processing, and extended reality systems. We are deeply grateful for all of these discussions and specifically to the following: Wei Cu, Aleksandra Faust, Liang Gao, Matt Horsnell, Amit Jindal, Steve LaValle, Steve Lovegrove, Andrew Maimone, Vegard \u00d8ye, Martin Persson, Archontis Politis, Eric Shaffer, Paris Smaragdis, Sachin Talathi, and Chris Widdowson. Our OpenXR implementation is derived from Monado . We are particularly thankful to Jakob Bornecrantz and Ryan Pavlik. The development of ILLIXR was supported by the Applications Driving Architectures (ADA) Research Center (a JUMP Center co-sponsored by SRC and DARPA), the Center for Future Architectures Research (C-FAR, a STARnet research center), a Semiconductor Research Corporation program sponsored by MARCO and DARPA, and by a Google Faculty Research Award. The development of ILLIXR was also aided by generous hardware and software donations from ARM and NVIDIA. Facebook Reality Labs provided the OpenEDS Semantic Segmentation Dataset . Wesley Darvin came up with the name for ILLIXR.","title":"Acknowledgements"},{"location":"#licensing-structure","text":"ILLIXR is available as open-source software under the permissive University of Illinois/NCSA Open Source License . As mentioned above, ILLIXR largely consists of components developed by domain experts and modified for the purposes of inclusion in ILLIXR. However, ILLIXR does contain software developed solely by us. The NCSA license is limited to only this software . The external libraries and softwares included in ILLIXR each have their own licenses and must be used according to those licenses: ElasticFusion \\ ElasticFusion license KinectFusion \\ MIT License GTSAM \\ Simplified BSD License HOTlab \\ GNU Lesser General Public License v3.0 Kimera-VIO \\ Simplified BSD License libspatialaudio \\ GNU Lesser General Public License v2.1 Monado \\ Boost Software License 1.0 moodycamel::ConcurrentQueue \\ Simplified BSD License Open-VINS \\ GNU General Public License v3.0 RITnet \\ MIT License Note that ILLIXR's extensibility allows the source to be configured and compiled using only permissively licensed software.","title":"Licensing Structure"},{"location":"#get-in-touch","text":"Whether you are a computer architect, a compiler writer, a systems person, work on XR related algorithms or applications, or just anyone interested in XR research, development, or products, we would love to hear from you and hope you will contribute! You can join the ILLIXR consortium , Discord , or mailing list , or send us an email , or just send us a pull request!","title":"Get in Touch"},{"location":"CONTRIBUTING/","text":"Contributing Guidelines Please follow these steps when making pull requests (PRs): First, create an issue describing the problem that needs to be fixed. If an issue already exists, skip this step. If you are looking for an issue to fix, see the \"good first issue\" label . Assign the issue to yourself and add appropriate labels. If you are an external contributor, comment on the issue so one of the ILLIXR team members can assign the issue to you. Before you start making changes, make a new branch. The branch MUST be named issue-<issue number>-<some descriptive name> . For instance, issue-32-fix-mem-leak addresses the memory leak described in Issue #32. Fix the issue. Add your name to ILLIXR/CONTRIBUTORS . Push commits up to GitHub. Open a PR, and link it to the issue that the PR aims to resolve. Please give the PR a descriptive name. As you make progress on your PR, keep your branch up-to-date with the master branch which may have been updated after starting your PR. Your PR MUST be updated to reflect changes to master in order to be merged. Use the following procedure for updating your branch and when you are ready to commit your changes: ## While on your PR branch <issue-branch> hosted at <your-remote> repository: git commit # or git stash ## Line A git checkout master git pull <illixr-remote> master --rebase && git fetch <illixr-remote> ## Line B git checkout <issue-branch> git rebase master ## Line C ## If you stashed your changes on 'Line A': git stash apply <stash-number> && git commit git push <your-remote> <issue-branch> --force-with-lease ## Line D For ILLIXR team members (others jump here ): In the example above, <illixr-remote> and <your-remote> are the same. When collaborating on branches in our repository, Line B may pull in changes that overwrite the git commit history when performing Line C . Subsequently, performing Line D will rewrite the history in the public branch. To preserve branch commit histories in the case that a rollback is needed, we will employ a checkpointing process for force updated branches. This process will be manually performed, but may be automated in the future. If Line B shows an update to master, the following example illustrates your local repository just after performing Line B : A -- B -- C -- P -- Q -- R ## master \\ D -- E -- F ## issue-123-fixing-bug In this example, commits P , Q , and R have been merged to master (from feature branches not shown) after feature branch issue-123-fixing-bug was forked from master . To checkpoint the issue-123-fixing-bug branch while it is checked out: git branch issue-123.0-fixing-bug ## Make alias for old issue-123-fixing-bug git checkout -b issue-123.1-fixing-bug ## Make new branch to rebase with master git rebase master ## Replay issue-123-fixing-bug onto master git branch -D issue-123-fixing-bug ## Remove old issue-123-fixing-bug git branch issue-123-fixing-bug ## Make issue-123-fixing-bug an alias of new branch git push <illixr-remote> issue-123.{0,1}-fixing-bug ## Push new checkpointed branches to remote git push <illixr-remote> issue-123-fixing-bug --force-with-lease ## Force update issue-123-fixing-bug Note: The term alias here is used to refer to branches which point to the same commit. This usage is different from standard Git Aliases used for git command shortcuts. After checkpointing, your local repository should look as follows: D' -- E' -- F' ## issue-123.1-fixing-bug, issue-123-fixing-bug / A -- B -- C -- P -- Q -- R ## master \\ D -- E -- F ## issue-123.0-fixing-bug Commits D , E , and F have been added to a new branch starting from R , but now have been given new hashes. This new branch is our up-to-date copy of the feature branch issue-123-fixing-bug . While working on a checkpointed branch, keep aliases up-to-date using git rebase : git commit ## Add changes to issue-123.1-fixing-bug git checkout issue-123-fixing-bug ## Switch to main issue-123-fixing-bug branch git rebase issue-123.1-fixing-bug ## Fast-forward issue-123-fixing-bug to issue-123.1-fixing-bug Conflicts are possible when two or more collaborators push changes concurrently to the same branch. As long as each collaborator ensures that the branch update process starts at Line A , conflicts can be detected and handled locally. In other words, every call to git-push should be preceeded by a call to git-pull , following the process from Line A to Line D (or equivalent; git's CLI allows many ways to achieve the same results). Note: Line B rebases the master branch assuming that we have checked out master . Forgetting to specify master in Line B may result in a lossy forced update in the example below. Forgetting to checkout master will immediately apply your checked out feature branch's changes, possibly also resulting in a lossy forced update. The output of Line B for a collaborator after the checkpointing process may contain something like this: From github.com:ILLIXR/ILLIXR A..R master -> <illixr-remote>/master + A..F' issue-123-fixing-bug -> <illixr-remote>/issue-123-fixing-bug (forced update) * [new branch] issue-123.0-fixing-bug -> <illixr-remote>/issue-123.0-fixing-bug * [new branch] issue-123.1-fixing-bug -> <illixr-remote>/issue-123.1-fixing-bug Conflicts which do not involve updates to the master branch can be resolved simply by rebasing the current feature branch with the updated feature branch, applying new changes on top of the updated feature branch: ## For the latest checkpoint X (local) and Y (remote), let Z := Y + 1 in git checkout issue-123.X-fixing-bug -b issue-123.Z-fixing-bug ## Make new branch issue-123.Z-fixing-bug git rebase <illixr-remote>/issue-123.Y-fixing-bug ## Replay updates from issue-123.X-fixing-bug git push <illixr-remote> issue-123.Z-fixing-bug ## Make sure to update issue-123-fixing-bug after The --force-with-lease argument in Line D is not required for our new checkpoint branch, since a new branch should not conflict with a non-existing remote branch. We expect the subversion number for a new branch resulting from our checkpoint conflict resolution to be new and unique. If the push fails, another conflict has occurred, and checkpoint conflict resolution should be repeated. Line D should be safe to perform for the main feature branch now that we have replayed our commits on top of the updated feature branch. Note: In the above example, the git-rebase is performed using the remote copy of the checkpointed branch. We do this because Line B will not fast-forward or force update our local branches (with the same subversion number as a conflicting remote branch, if any). In the case of a conflict with updates to master , Line A should show updates to both the master branch and the feature branch to be pushed in Line D . A checkpointed version of the feature branch may also appear. This is because a feature branch should only be checkpointed in the presence of a change to the master branch. Forced pushes should generally not be used for any other purpose. If multiple updates to master and the feature branch have occured, additional checkpointed versions of the feature branch may also appear. In this scenario, we need to rebase our latest version of the feature branch with the latest version of the feature branch pulled from <illixr-remote> . Philosophy Why are the above steps necessary? Assigning the issue to yourself ensures that multiple people don't work on the same thing in parallel. The branch naming scheme organizes things a bit for us, and also makes it easy to find branches. Linking the issue to the PR ensures that we know which issue is being resolved, and also automatically closes the issue when the PR gets merged. Using rebases keeps the master and feature branch histories streamlined (minimizing branching), thus making it easier to compose feature branches for integration testing. See this article on rebasing public branches for more information. If your PR has not seen activity from the ILLIXR team after a long period of time (e.g., 2 weeks), feel free to contact the team directly on the GitHub Issue Conversation tab or at the Gitter forum linked below. Other Procedures Branch Management: The branch rebasing and checkpointing process detailed above is tedious, and may be automated in the future. Check back in with this document occasionally for improvements to the branch management process. Code Formatting: As ILLIXR grows, contributions will need to be standardized to accomodate multiple collaborators with different coding styles. During code review of a PR, you may be asked to reformat your code to match the standards set for ILLIXR code base. This process may be manually triggered by a comment from a review, or automated via Git and GitHub in the future. Issue Templates: To make collaboration easier, templates for Issues and Pull Requests will be added to the GitHub web interface. If an appropriate template exists for your task, please ensure to select it before submitting. Getting Help You can get seek help from our development community in three places: Main documentation site API documentation site Gitter community forum","title":"Contributing Guidelines (Version 2)"},{"location":"CONTRIBUTING/#contributing-guidelines","text":"Please follow these steps when making pull requests (PRs): First, create an issue describing the problem that needs to be fixed. If an issue already exists, skip this step. If you are looking for an issue to fix, see the \"good first issue\" label . Assign the issue to yourself and add appropriate labels. If you are an external contributor, comment on the issue so one of the ILLIXR team members can assign the issue to you. Before you start making changes, make a new branch. The branch MUST be named issue-<issue number>-<some descriptive name> . For instance, issue-32-fix-mem-leak addresses the memory leak described in Issue #32. Fix the issue. Add your name to ILLIXR/CONTRIBUTORS . Push commits up to GitHub. Open a PR, and link it to the issue that the PR aims to resolve. Please give the PR a descriptive name. As you make progress on your PR, keep your branch up-to-date with the master branch which may have been updated after starting your PR. Your PR MUST be updated to reflect changes to master in order to be merged. Use the following procedure for updating your branch and when you are ready to commit your changes: ## While on your PR branch <issue-branch> hosted at <your-remote> repository: git commit # or git stash ## Line A git checkout master git pull <illixr-remote> master --rebase && git fetch <illixr-remote> ## Line B git checkout <issue-branch> git rebase master ## Line C ## If you stashed your changes on 'Line A': git stash apply <stash-number> && git commit git push <your-remote> <issue-branch> --force-with-lease ## Line D For ILLIXR team members (others jump here ): In the example above, <illixr-remote> and <your-remote> are the same. When collaborating on branches in our repository, Line B may pull in changes that overwrite the git commit history when performing Line C . Subsequently, performing Line D will rewrite the history in the public branch. To preserve branch commit histories in the case that a rollback is needed, we will employ a checkpointing process for force updated branches. This process will be manually performed, but may be automated in the future. If Line B shows an update to master, the following example illustrates your local repository just after performing Line B : A -- B -- C -- P -- Q -- R ## master \\ D -- E -- F ## issue-123-fixing-bug In this example, commits P , Q , and R have been merged to master (from feature branches not shown) after feature branch issue-123-fixing-bug was forked from master . To checkpoint the issue-123-fixing-bug branch while it is checked out: git branch issue-123.0-fixing-bug ## Make alias for old issue-123-fixing-bug git checkout -b issue-123.1-fixing-bug ## Make new branch to rebase with master git rebase master ## Replay issue-123-fixing-bug onto master git branch -D issue-123-fixing-bug ## Remove old issue-123-fixing-bug git branch issue-123-fixing-bug ## Make issue-123-fixing-bug an alias of new branch git push <illixr-remote> issue-123.{0,1}-fixing-bug ## Push new checkpointed branches to remote git push <illixr-remote> issue-123-fixing-bug --force-with-lease ## Force update issue-123-fixing-bug Note: The term alias here is used to refer to branches which point to the same commit. This usage is different from standard Git Aliases used for git command shortcuts. After checkpointing, your local repository should look as follows: D' -- E' -- F' ## issue-123.1-fixing-bug, issue-123-fixing-bug / A -- B -- C -- P -- Q -- R ## master \\ D -- E -- F ## issue-123.0-fixing-bug Commits D , E , and F have been added to a new branch starting from R , but now have been given new hashes. This new branch is our up-to-date copy of the feature branch issue-123-fixing-bug . While working on a checkpointed branch, keep aliases up-to-date using git rebase : git commit ## Add changes to issue-123.1-fixing-bug git checkout issue-123-fixing-bug ## Switch to main issue-123-fixing-bug branch git rebase issue-123.1-fixing-bug ## Fast-forward issue-123-fixing-bug to issue-123.1-fixing-bug Conflicts are possible when two or more collaborators push changes concurrently to the same branch. As long as each collaborator ensures that the branch update process starts at Line A , conflicts can be detected and handled locally. In other words, every call to git-push should be preceeded by a call to git-pull , following the process from Line A to Line D (or equivalent; git's CLI allows many ways to achieve the same results). Note: Line B rebases the master branch assuming that we have checked out master . Forgetting to specify master in Line B may result in a lossy forced update in the example below. Forgetting to checkout master will immediately apply your checked out feature branch's changes, possibly also resulting in a lossy forced update. The output of Line B for a collaborator after the checkpointing process may contain something like this: From github.com:ILLIXR/ILLIXR A..R master -> <illixr-remote>/master + A..F' issue-123-fixing-bug -> <illixr-remote>/issue-123-fixing-bug (forced update) * [new branch] issue-123.0-fixing-bug -> <illixr-remote>/issue-123.0-fixing-bug * [new branch] issue-123.1-fixing-bug -> <illixr-remote>/issue-123.1-fixing-bug Conflicts which do not involve updates to the master branch can be resolved simply by rebasing the current feature branch with the updated feature branch, applying new changes on top of the updated feature branch: ## For the latest checkpoint X (local) and Y (remote), let Z := Y + 1 in git checkout issue-123.X-fixing-bug -b issue-123.Z-fixing-bug ## Make new branch issue-123.Z-fixing-bug git rebase <illixr-remote>/issue-123.Y-fixing-bug ## Replay updates from issue-123.X-fixing-bug git push <illixr-remote> issue-123.Z-fixing-bug ## Make sure to update issue-123-fixing-bug after The --force-with-lease argument in Line D is not required for our new checkpoint branch, since a new branch should not conflict with a non-existing remote branch. We expect the subversion number for a new branch resulting from our checkpoint conflict resolution to be new and unique. If the push fails, another conflict has occurred, and checkpoint conflict resolution should be repeated. Line D should be safe to perform for the main feature branch now that we have replayed our commits on top of the updated feature branch. Note: In the above example, the git-rebase is performed using the remote copy of the checkpointed branch. We do this because Line B will not fast-forward or force update our local branches (with the same subversion number as a conflicting remote branch, if any). In the case of a conflict with updates to master , Line A should show updates to both the master branch and the feature branch to be pushed in Line D . A checkpointed version of the feature branch may also appear. This is because a feature branch should only be checkpointed in the presence of a change to the master branch. Forced pushes should generally not be used for any other purpose. If multiple updates to master and the feature branch have occured, additional checkpointed versions of the feature branch may also appear. In this scenario, we need to rebase our latest version of the feature branch with the latest version of the feature branch pulled from <illixr-remote> .","title":"Contributing Guidelines"},{"location":"CONTRIBUTING/#philosophy","text":"Why are the above steps necessary? Assigning the issue to yourself ensures that multiple people don't work on the same thing in parallel. The branch naming scheme organizes things a bit for us, and also makes it easy to find branches. Linking the issue to the PR ensures that we know which issue is being resolved, and also automatically closes the issue when the PR gets merged. Using rebases keeps the master and feature branch histories streamlined (minimizing branching), thus making it easier to compose feature branches for integration testing. See this article on rebasing public branches for more information. If your PR has not seen activity from the ILLIXR team after a long period of time (e.g., 2 weeks), feel free to contact the team directly on the GitHub Issue Conversation tab or at the Gitter forum linked below.","title":"Philosophy"},{"location":"CONTRIBUTING/#other-procedures","text":"Branch Management: The branch rebasing and checkpointing process detailed above is tedious, and may be automated in the future. Check back in with this document occasionally for improvements to the branch management process. Code Formatting: As ILLIXR grows, contributions will need to be standardized to accomodate multiple collaborators with different coding styles. During code review of a PR, you may be asked to reformat your code to match the standards set for ILLIXR code base. This process may be manually triggered by a comment from a review, or automated via Git and GitHub in the future. Issue Templates: To make collaboration easier, templates for Issues and Pull Requests will be added to the GitHub web interface. If an appropriate template exists for your task, please ensure to select it before submitting.","title":"Other Procedures"},{"location":"CONTRIBUTING/#getting-help","text":"You can get seek help from our development community in three places: Main documentation site API documentation site Gitter community forum","title":"Getting Help"},{"location":"LICENSE/","text":"Copyright (c) 2019 The Board of Trustees of the University of Illinois. All rights reserved. Developed by: Professor Sarita Adve's research group University of Illinois at Urbana-Champaign http://rsim.cs.illinois.edu Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution. * Neither the names of Professor Sarita Adve's research group, University of Illinois at Urbana-Champaign, nor the names of its contributors may be used to endorse or promote products derived from this Software without specific prior written permission. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.","title":"ILLIXR License (Version 2)"},{"location":"building_illixr/","text":"Building ILLIXR The ILLIXR application is kick-started through a tool called Runner (found in runner/ and runner.sh ). The Runner tool is responsible for preparing the environment, downloading required assets/code, compiling each plugin, and launching the ILLIXR application. Runner is necessary for our project since ILLIXR manages plugins and data that span many locations and launch configurations . A configuration (defined via a YAML file in ILLIXR/configs/ ) specifies parameters and plugins required to launch ILLIXR for a specific design/evaluation scenario. Compilation and Usage To run ILLIXR (from the root directory of the project) using the default native launch configuration, ./runner.sh configs/native.yaml To drop into gdb , add command: gdb -q --args $cmd in the action block of configs/native.yaml , and use the same command. To run ILLIXR with Monado, ./runner.sh configs/monado.yaml The OpenXR application to run is defined in the action.openxr_app (a YAML object). Configuration As introduced in the introduction to the ILLIXR build process , a Configuration (or config ) describes the key information needed to launch an ILLIXR application. This section provides a detailed breakdown of the structure of a configuration file. The default ILLIXR/configs/native.yaml for the native action will be used as the running example. The first block in the config file contains a list of plugin_groups , where each plugin_group is a list of plugins. plugin_groups: - plugin_group: - path: plugin1/ - path: plugin2/ - path: plugin3/ - path: plugin4/ This defines a list of plugins by their location, path . Allowed paths will be described below. The plugin_groups get flattened and those plugins are initialized in order at runtime. Several of the default plugins are order-sensitive. The next block in the config defines the offline IMU data, camera data, and ground-truth data. data: subpath: mav0 relative_to: archive_path: download_url: 'http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room1/V1_02_medium/V1_02_medium.zip' Next, we define the location of OBJ files for gldemo . demo_data: demo_data/ Then, we define the Action to be taken for the configuration. Each action has a name, and can contain a number of member fields beyond this. action: name: native command: gdb -q --args $cmd The native action supports an optional command argument. In that argument $cmd is replaced with the separated command-line arguments to run ILLIXR, while $quoted_cmd is replaced with a single string comprising all command-line arguments. The command argument also supports $env_cmd , which interpret command-line argument assignments in the form of VARNAME=VALUE as environment variable mappings. See the configuration glossary entry for more details about supported actions. Finally, we support two compilation profiles : opt , which compiles with -O3 and disables debug prints and assertions, and dbg , which compiles with debug flags and enables debug prints and assertions. profile: opt You can !include other configuration files via pyyaml-include . Consider separating the site-specific configuration options into its own file. Specifying Paths A path refers to a location of a resource. There are 5 ways of specifying a path: Simple path : Either absolute or relative path in the native filesystem. Git repo : A git repository. - git_repo: https://github.com/user/repo.git version: master # branch name, SHA-256, or tag Download URL : A resource downloaded from the internet. - download_url: https://example.com/file.txt Zip archive : A path that points within the contents of a zip archive. Note that archive_path is itself a path (recursive). - archive_path: path/to/archive.zip - archive_path: download_url: https://example.com/file.zip Complex path : A hard-coded path relative to another path (recursive). This is useful to specify a subdirectory of a git repository or zip archive. - subpath: path/within/git_repo relative_to: git_repo: ... version: ... Rationale Previously, we would have to specify which plugins to build and which to run separately, violating DRY principle . Previously, configuration had to be hard-coded into the component source code, or passed as parsed/unparsed as strings in env-vars on a per-component basis. This gives us a consistent way to deal with all configurations. Currently, plugins are specified by a path to the directory containing their source code and build system. Philosophy Each plugin should not have to know or care how the others are compiled. In the future, they may even be distributed separately, just as SOs. Therefore, each plugin needs its own build system. Despite this per-plugin flexibility, building the 'default' set of ILLIXR plugins should be extremely easy. It should be easy to build in parallel. Always rebuild every time, so the binary is always \"fresh.\" This is a great convenience when experimenting. However, this implies that rebuilding must be fast when not much has changed. Make is the de facto standard for building C/C++ programs. GNU Make, and the makefile language begets no shortage of problems [ 1 , 2 , 3 , 4 , 5 ], but we choose Make for its tradeoff of between simplicity and functionality. What it lacks in functionality (compared to CMake, Ninja, scons, Bazel, Meson) it makes up for in simplicity. It's still the build system in which it is the easiest to invoke arbitrary commands in shell and the easiest to have a common.mk included in each plugin. This decision to use Make should be revisited, when this project outgrows its ability, but for now, Make remains, in our judgement, the best tool for the job .","title":"Building ILLIXR (Version 2)"},{"location":"building_illixr/#building-illixr","text":"The ILLIXR application is kick-started through a tool called Runner (found in runner/ and runner.sh ). The Runner tool is responsible for preparing the environment, downloading required assets/code, compiling each plugin, and launching the ILLIXR application. Runner is necessary for our project since ILLIXR manages plugins and data that span many locations and launch configurations . A configuration (defined via a YAML file in ILLIXR/configs/ ) specifies parameters and plugins required to launch ILLIXR for a specific design/evaluation scenario.","title":"Building ILLIXR"},{"location":"building_illixr/#compilation-and-usage","text":"To run ILLIXR (from the root directory of the project) using the default native launch configuration, ./runner.sh configs/native.yaml To drop into gdb , add command: gdb -q --args $cmd in the action block of configs/native.yaml , and use the same command. To run ILLIXR with Monado, ./runner.sh configs/monado.yaml The OpenXR application to run is defined in the action.openxr_app (a YAML object).","title":"Compilation and Usage"},{"location":"building_illixr/#configuration","text":"As introduced in the introduction to the ILLIXR build process , a Configuration (or config ) describes the key information needed to launch an ILLIXR application. This section provides a detailed breakdown of the structure of a configuration file. The default ILLIXR/configs/native.yaml for the native action will be used as the running example. The first block in the config file contains a list of plugin_groups , where each plugin_group is a list of plugins. plugin_groups: - plugin_group: - path: plugin1/ - path: plugin2/ - path: plugin3/ - path: plugin4/ This defines a list of plugins by their location, path . Allowed paths will be described below. The plugin_groups get flattened and those plugins are initialized in order at runtime. Several of the default plugins are order-sensitive. The next block in the config defines the offline IMU data, camera data, and ground-truth data. data: subpath: mav0 relative_to: archive_path: download_url: 'http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room1/V1_02_medium/V1_02_medium.zip' Next, we define the location of OBJ files for gldemo . demo_data: demo_data/ Then, we define the Action to be taken for the configuration. Each action has a name, and can contain a number of member fields beyond this. action: name: native command: gdb -q --args $cmd The native action supports an optional command argument. In that argument $cmd is replaced with the separated command-line arguments to run ILLIXR, while $quoted_cmd is replaced with a single string comprising all command-line arguments. The command argument also supports $env_cmd , which interpret command-line argument assignments in the form of VARNAME=VALUE as environment variable mappings. See the configuration glossary entry for more details about supported actions. Finally, we support two compilation profiles : opt , which compiles with -O3 and disables debug prints and assertions, and dbg , which compiles with debug flags and enables debug prints and assertions. profile: opt You can !include other configuration files via pyyaml-include . Consider separating the site-specific configuration options into its own file.","title":"Configuration"},{"location":"building_illixr/#specifying-paths","text":"A path refers to a location of a resource. There are 5 ways of specifying a path: Simple path : Either absolute or relative path in the native filesystem. Git repo : A git repository. - git_repo: https://github.com/user/repo.git version: master # branch name, SHA-256, or tag Download URL : A resource downloaded from the internet. - download_url: https://example.com/file.txt Zip archive : A path that points within the contents of a zip archive. Note that archive_path is itself a path (recursive). - archive_path: path/to/archive.zip - archive_path: download_url: https://example.com/file.zip Complex path : A hard-coded path relative to another path (recursive). This is useful to specify a subdirectory of a git repository or zip archive. - subpath: path/within/git_repo relative_to: git_repo: ... version: ...","title":"Specifying Paths"},{"location":"building_illixr/#rationale","text":"Previously, we would have to specify which plugins to build and which to run separately, violating DRY principle . Previously, configuration had to be hard-coded into the component source code, or passed as parsed/unparsed as strings in env-vars on a per-component basis. This gives us a consistent way to deal with all configurations. Currently, plugins are specified by a path to the directory containing their source code and build system.","title":"Rationale"},{"location":"building_illixr/#philosophy","text":"Each plugin should not have to know or care how the others are compiled. In the future, they may even be distributed separately, just as SOs. Therefore, each plugin needs its own build system. Despite this per-plugin flexibility, building the 'default' set of ILLIXR plugins should be extremely easy. It should be easy to build in parallel. Always rebuild every time, so the binary is always \"fresh.\" This is a great convenience when experimenting. However, this implies that rebuilding must be fast when not much has changed. Make is the de facto standard for building C/C++ programs. GNU Make, and the makefile language begets no shortage of problems [ 1 , 2 , 3 , 4 , 5 ], but we choose Make for its tradeoff of between simplicity and functionality. What it lacks in functionality (compared to CMake, Ninja, scons, Bazel, Meson) it makes up for in simplicity. It's still the build system in which it is the easiest to invoke arbitrary commands in shell and the easiest to have a common.mk included in each plugin. This decision to use Make should be revisited, when this project outgrows its ability, but for now, Make remains, in our judgement, the best tool for the job .","title":"Philosophy"},{"location":"debugging_illixr/","text":"ILLIXR Debugging Tips Debugging Locally The config described in Building ILLIXR supports running the runtime with arbitrary commands like gdb . When debugging locally, we recommend using either gdb or valgrind in this way. Debugging Pull Requests or with a Clean Environment 1. Get a Docker Image From your Local Project From the root directory in your project, run: docker build [--build-arg=JOBS=\"<integer>\"] [--no-cache] --tag <repository>:<tag> . Note the optional Dockerfile argument, JOBS , which specifies the number of threads/tasks to use for building. Also note the optional argument, --no-cache , which forces Docker to rerun commands in Dockerfile (see this article for more information). For this project's main module, you can use something like illixr-illixr for the <repository> value, and your current branch name or release version as the <tag> value. Note that building the docker image can take some time (up to 40min on a 4-core desktop machine) and uses somewhere between 2-4GB of RAM. From a GitHub Pull Request's CI/CD Flow Follow these steps when a CI/CD build fails on a PR: Click details on the failing build. In the build view go to the Push Docker Image tab and copy the docker push ghcr.io/illixr/illixr-tests:<branch-name> command. Then in your terminal, run: docker pull ghcr.io/illixr/illixr-tests:<branch-name> 2. Test your Image in the Docker container Verify that your image was created successfully: docker image ls Take note of your image's REPOSITORY and TAG values. Now run: docker run -it --entrypoint /bin/bash <repository>:<tag> You are now in a bash shell in a docker container. From here you can test whichever project flow you wish, such as the usual ./runner.sh configs/native.yaml , or the CI/CD testing flow ( ./runner.sh configs/ci.yaml ).","title":"ILLIXR Debugging Tips (Version 2)"},{"location":"debugging_illixr/#illixr-debugging-tips","text":"","title":"ILLIXR Debugging Tips"},{"location":"debugging_illixr/#debugging-locally","text":"The config described in Building ILLIXR supports running the runtime with arbitrary commands like gdb . When debugging locally, we recommend using either gdb or valgrind in this way.","title":"Debugging Locally"},{"location":"debugging_illixr/#debugging-pull-requests-or-with-a-clean-environment","text":"","title":"Debugging Pull Requests or with a Clean Environment"},{"location":"debugging_illixr/#1-get-a-docker-image","text":"","title":"1. Get a Docker Image"},{"location":"debugging_illixr/#from-your-local-project","text":"From the root directory in your project, run: docker build [--build-arg=JOBS=\"<integer>\"] [--no-cache] --tag <repository>:<tag> . Note the optional Dockerfile argument, JOBS , which specifies the number of threads/tasks to use for building. Also note the optional argument, --no-cache , which forces Docker to rerun commands in Dockerfile (see this article for more information). For this project's main module, you can use something like illixr-illixr for the <repository> value, and your current branch name or release version as the <tag> value. Note that building the docker image can take some time (up to 40min on a 4-core desktop machine) and uses somewhere between 2-4GB of RAM.","title":"From your Local Project"},{"location":"debugging_illixr/#from-a-github-pull-requests-cicd-flow","text":"Follow these steps when a CI/CD build fails on a PR: Click details on the failing build. In the build view go to the Push Docker Image tab and copy the docker push ghcr.io/illixr/illixr-tests:<branch-name> command. Then in your terminal, run: docker pull ghcr.io/illixr/illixr-tests:<branch-name>","title":"From a GitHub Pull Request's CI/CD Flow"},{"location":"debugging_illixr/#2-test-your-image-in-the-docker-container","text":"Verify that your image was created successfully: docker image ls Take note of your image's REPOSITORY and TAG values. Now run: docker run -it --entrypoint /bin/bash <repository>:<tag> You are now in a bash shell in a docker container. From here you can test whichever project flow you wish, such as the usual ./runner.sh configs/native.yaml , or the CI/CD testing flow ( ./runner.sh configs/ci.yaml ).","title":"2. Test your Image in the Docker container"},{"location":"getting_started/","text":"Getting Started These instructions have been tested with Ubuntu 18.04 and 20.04. ILLIXR Runtime without Monado Clone the repository : git clone --recursive --branch v2-latest https://github.com/ILLIXR/ILLIXR Note for ILLIXR versions older than v2.2.0 : Update the submodules. Submodules are git repositories inside a git repository that need to be pulled down separately: git submodule update --init --recursive Install dependencies : ./install_deps.sh [--jobs <integer>] This script installs some Ubuntu/Debian packages and builds several dependencies from source. Without any arguments, the script will print the help message, and proceed using default values. To change the number of threads/tasks to use for building, specify using the --jobs argument. Other available options can be inspected using the --help flag. Inspect configs/native.yaml . The schema definition is in runner/config_schema.yaml . For more details on the runner and the config files, see Building ILLIXR . Build and run ILLIXR without Monado : ./runner.sh configs/native.yaml If you are running ILLIXR without a graphical environment, try ILLIXR headlessly using Xvfb : ./runner.sh configs/headless.yaml To clean up after building, run : ./runner.sh configs/clean.yaml Or simply: ./clean.sh ILLIXR Runtime with Monado ILLIXR leverages Monado , an open-source implementation of OpenXR , to support a wide range of OpenXR client applications. Because of a low-level driver issue, Monado only supports Ubuntu 18.04+. Compile and run: ./runner.sh configs/monado.yaml ILLIXR under Virtualization ILLIXR can be run inside a QEMU-KVM image. Check out the instructions here . Next Steps Try browsing the source for the runtime and provided plugins. The source code is divided into components in the following directories: ILLIXR/runtime/ : A directory holding the implementation for loading and interfacing plugins. This directory contains Spindle . ILLIXR/common/ : A directory holding resources and utilities available globally to all plugins. Most plugins symlink this directory into theirs. This directory contains the interfaces for Switchboard and Phonebook . ILLIXR/<plugin_dir>/ : A unique directory for each plugin. Most of the core XR functionality is implemented via plugins. See Default Components for more details. If you edit any of the source files, the runner will detect and rebuild the respective binary the next time it runs. If you want to add your own plugin, see Writing Your Plugin . Otherwise, proceed to the next section, Building ILLIXR .","title":"Getting Started (Version 2)"},{"location":"getting_started/#getting-started","text":"These instructions have been tested with Ubuntu 18.04 and 20.04.","title":"Getting Started"},{"location":"getting_started/#illixr-runtime-without-monado","text":"Clone the repository : git clone --recursive --branch v2-latest https://github.com/ILLIXR/ILLIXR Note for ILLIXR versions older than v2.2.0 : Update the submodules. Submodules are git repositories inside a git repository that need to be pulled down separately: git submodule update --init --recursive Install dependencies : ./install_deps.sh [--jobs <integer>] This script installs some Ubuntu/Debian packages and builds several dependencies from source. Without any arguments, the script will print the help message, and proceed using default values. To change the number of threads/tasks to use for building, specify using the --jobs argument. Other available options can be inspected using the --help flag. Inspect configs/native.yaml . The schema definition is in runner/config_schema.yaml . For more details on the runner and the config files, see Building ILLIXR . Build and run ILLIXR without Monado : ./runner.sh configs/native.yaml If you are running ILLIXR without a graphical environment, try ILLIXR headlessly using Xvfb : ./runner.sh configs/headless.yaml To clean up after building, run : ./runner.sh configs/clean.yaml Or simply: ./clean.sh","title":"ILLIXR Runtime without Monado"},{"location":"getting_started/#illixr-runtime-with-monado","text":"ILLIXR leverages Monado , an open-source implementation of OpenXR , to support a wide range of OpenXR client applications. Because of a low-level driver issue, Monado only supports Ubuntu 18.04+. Compile and run: ./runner.sh configs/monado.yaml","title":"ILLIXR Runtime with Monado"},{"location":"getting_started/#illixr-under-virtualization","text":"ILLIXR can be run inside a QEMU-KVM image. Check out the instructions here .","title":"ILLIXR under Virtualization"},{"location":"getting_started/#next-steps","text":"Try browsing the source for the runtime and provided plugins. The source code is divided into components in the following directories: ILLIXR/runtime/ : A directory holding the implementation for loading and interfacing plugins. This directory contains Spindle . ILLIXR/common/ : A directory holding resources and utilities available globally to all plugins. Most plugins symlink this directory into theirs. This directory contains the interfaces for Switchboard and Phonebook . ILLIXR/<plugin_dir>/ : A unique directory for each plugin. Most of the core XR functionality is implemented via plugins. See Default Components for more details. If you edit any of the source files, the runner will detect and rebuild the respective binary the next time it runs. If you want to add your own plugin, see Writing Your Plugin . Otherwise, proceed to the next section, Building ILLIXR .","title":"Next Steps"},{"location":"glossary/","text":"Glossary of ILLIXR Terminology A collection of ILLIXR and ILLIXR-adjacent terms and their definitions can be found on this page your reference. General Runtime The ILLIXR system runtime is responsible for the dynamic orchestration of ILLIXR device resources, system resources, and client applications. The runtime implementation is located in ILLIXR/runtime/ . See the Building ILLIXR and Monado Overiew pages for details about the ILLIXR runtime. Plugin A modular component that can be detected and enabled for use by an ILLIXR application. A plugin can be internal or external to the ILLIXR project . Each plugin is compiled and launched dynamically at runtime based on the ILLIXR configuration used. ILLIXR also implements a Monado runtime translation Plugin . For a list of supported plugins and their details, see the ILLIXR Plugins page. For instructions for how to modify or write your own plugins, see the Modifying a Plugin and Writing Your Plugin pages. See the Plugin API documentation . Config(uration) A file describing the key information required to launch an ILLIXR application. Configurations for ILLIXR are implemented as YAML files. Each configuration comprises an action , a profile , and a list of plugins as defined by our configuration specification Schema . Action (Previously Loader) : An action encapsulates a task for Runner . native : The default application launch configuration. Does not use our Monado runtime integration. Defined in ILLIXR/configs/native.yaml . native-lookup : Same as native , but using a ground truth lookup from a file for the pose instead of computing it. Defined in ILLIXR/configs/native-lookup.yaml . headless : Same as native , but using Xvfb to run without a graphical environment. Defined in ILLIXR/configs/headless.yaml . ci : Same as headless , but using Docker virtualization and debug-enabled compilation. Defined in ILLIXR/configs/ci.yaml . monado : Similar to native , but uses our Monado runtime integration. Defined in ILLIXR/configs/monado.yaml . clean : A meta-task that fetches all supported plugins and then cleans up builds across the entire ILLIXR project. Defined in ILLIXR/configs/clean.yaml and supported by ILLIXR/clean.sh . docs : A meta-task that generates and populates the documention subdirectories in the project. Profile : A profile captures the compilation mode used by Runner . opt : Sets Runner to compile the ILLIXR application and plugins with optimizations. dbg : Sets Runner to compile the ILLIXR application and plugins without optimizations, while enabling debug logic and debug logging. Schema : A schema captures the specification describing the allowable structure of a configuration file. Our schema is implemented using the json-schema specification . Defined in ILLIXR/runner/config_schema.yaml . For more details about the structure of a configuration, see the Building ILLIXR page . Framebuffer A region of memory used to hold graphical information to be output to a display or graphics device. Frame : A single frame (image) to be output to a display at a certain instant of time based on the system's frame rate . Frame Rate : The interval period between complete (as defined by the output resolution) frame updates and refreshes. In many systems, the target frame rate is determined by a fixed vertical sync ( VSYNC ) period. Depth Buffer : A framebuffer representing the depth information of a 3D scene. Depth information is useful for applications such as graphics and SLAM . Eye Buffer : A framebuffer dedicated for display through a HMD lens to be perceived by a user's eye. For more information, see the Wikipedia article . Swap Chain A set of virtual framebuffers to be output to a display. Only one framebuffer in a swap chain is displayed at a time, enabling the other virtual framebuffers to be concurrently modified in memory. For more information, see the Wikipedia article . Compositor A window manager that establishes a framebuffer for each window of a graphical system. A compositor merges information across its windows to construct a unified framebuffer. For more information, see the Wikipedia article . Head-mounted Display A display device worn on the head and face for use with VR and XR applications. Also known as a HMD . For more information, see the Wikipedia article . Eye Tracking The process of measuring the eye movement of a user (who is possibly also wearing a HMD ). For more information, see the Wikipedia article . Event Stream A communication interface supporting writes, sychronous reads, and asynchronous reads. For synchronous reads, every value written to the stream is visible to consumers. For asynchronous reads, only the latest values written are guaranteed to be visible to consumers. Pose The combination of orientation and position of an object, used for computer vision and robotics applications. ILLIXR applications make use of poses to track the user's HMD within the virtual environment. Internally, ILLIXR has multiple classifications of poses which are used for various purposes. Slow Pose : A slow pose is a ... TODO Fast Pose : A fast pose is a ... TODO True Pose : A true pose is a ... TODO Depracated starting ILLIXR release v2.X.X . Pose Prediction : To improve the user's perception latency experience the time between, pose prediction uses history and current system information to pre-compute the user's next pose Pre-computing the next pose allows for components downstream from the pose output in the event stream dataflow graph to begin computation. Pose Prediction is implemented in the pose_prediction ILLIXR plugin . For more information, see the Wikipedia article . Ground Truth The most accurate source of measurement available for a data set. Typically, ground truth measurements are provided for the evaluation of sensor data where the sensor or other data source is not as accurate or reliable as the source for the ground truth. Ground Truth Poses : A collection of poses used to evaluate the accuracy of pose generation and prediction algorithms. Ground Truth Images : A collection of images used to evaluate the accuracy of visual processing algorithms, like SLAM and VIO . See the ILLIXR Plugins page for information about sensors implemented in ILLIXR. Inertial Measurement Unit A device that reports its orientation in space and any forces applied it. Also known as an IMU . An IMU is implemented in the offline_imu_cam ILLIXR plugin . For more information, see the Wikipedia article . Simultaneous Localization and Mapping The computational process of creating a map of an unknown environment, and finding one's location within that space. Also known as SLAM . For more information, see the Wikipedia article . Visual Interial Odometry The process of computing a pose estimate from incoming visual information and measurements from the IMU . Also known as VIO . Often used in combination with SLAM techniques. See the Wikipedia article . Asynchronous Reprojection The processing of rendered video for motion interpolation. Asynchronous reprojection improves the perception of the rendered video to the HMD when rendering misses it target frame rate . Asynchronous reprojection is implemented in the timewarpgl ILLIXR plugin . See the Wikipedia article . Distortion Correction The processing of visual anomalies in images where rectilinear features have been warped. For more information, see the Wikipedia artice . Chromatic Abberation Correction The processing of visual anomalies in images where colors are diffracted due to imperfect optics or other perturbing factors. For more information, see the Wikipedia article . Components Runner An ILLIXR tool responsible for preparing the environment, downloading required assets & code, compiling each plugin, and launching the ILLIXR application. The implementation resides in ILLIXR/runner/ , and can be launched with the appropriate environment setup via ILLIXR/runner.sh . Action (Previously Loader) : See Configuration . Spindle An ILLIXR component responsible for launching and managing plugin threads. The implementation resides in ILLIXR/runtime/ . See the Spindle API documentation . Phonebook An ILLIXR service directory used to introspectively interface plugins and their data. The implementation resides in ILLIXR/runtime/ . See the Phonebook API documentation . Switchboard An ILLIXR event stream manager that maintains data pipelines between plugins. The implementation resides in ILLIXR/runtime/ . See the Switchboard API documentation . Technologies OpenXR An open standard for Augmented and Virtual Reality. ILLIXR components target the OpenXR standard and interact with the ILLIXR device via the Application Interface. For more information, visit the official site from the Khronos Group . Monado An open source, modular implementation of the OpenXR standard for GNU/Linux . See the ILLIXR Monado Overview and Monado Dataflow pages for details about our runtime integration using Monado. For more information, visit the official Monado development site . Godot An open source game development engine. ILLIXR applications targeting the OpenXR use Godot to access the engine's integration with the OpenXR standard via Monado . For more information, visit the official Godot site . Xvfb A virtual framebuffer for the X11 Window Sytem . ILLIXR uses Xvfb to enable running the graphical ILLIXR application without requiring the user to have a graphical environment configured at application launch. For more information, see the Xfvb man page . Docker A platform and containerization framework for deploying applications under virtualization. ILLIXR uses Docker to deploy and test code in a continuous integration and deployment pipeline. For more information, see the Docker overview and getting started page . QEMU-KVM An open source virtulization tool and machine emulator. See the instructions for running ILLIXR under Virtualization . For more information, see the official QEMU page . YAML A markup language and data serilization standard designed to be user friendly. We make use of the PyYAML and pyyaml-include libraries to implement our Configuration implementation. For more information, visit the official YAML page . SQLite A SQL database engine implementation in C designed to be lightweight and easy to use. The ILLIXR project allows user to records application statistics to a local database for efficient processing. See the Logging and Metrics page for usage details. For more information, see the SQLite development site . Vulkan A cross-platform graphics API that allows developers to efficiently target low-level hardware features. For more information, see the official Vulkan page from the Khronos Group . OpenGL A cross-platform graphics API that allows developers to create graphics applications easily and portably. Also known as GL . GL Context : A data structure storing the state of an OpenGL application instance. Within a GL context resides framebuffer data. It is not thread safe to share contexts without appropriate synchronization. GLFW : An open source implementation of OpenGL. Supports Windows, MacOS and, Linux ( X11 and Wayland). See the GLFW development site . For more information, see the official OpenGL page from the Khronos Group . Ubuntu An open source GNU/Linux operating system and distribution. ILLIXR currently supports the Long Term Support (LTS) versions of Ubuntu: 18.04 LTS (Bionic) and 20.04 LTS (Focal). For more information, visit the official Ubuntu site .","title":"ILLIXR Glossary (Version 2)"},{"location":"glossary/#glossary-of-illixr-terminology","text":"A collection of ILLIXR and ILLIXR-adjacent terms and their definitions can be found on this page your reference.","title":"Glossary of ILLIXR Terminology"},{"location":"glossary/#general","text":"","title":"General"},{"location":"glossary/#runtime","text":"The ILLIXR system runtime is responsible for the dynamic orchestration of ILLIXR device resources, system resources, and client applications. The runtime implementation is located in ILLIXR/runtime/ . See the Building ILLIXR and Monado Overiew pages for details about the ILLIXR runtime.","title":"Runtime"},{"location":"glossary/#plugin","text":"A modular component that can be detected and enabled for use by an ILLIXR application. A plugin can be internal or external to the ILLIXR project . Each plugin is compiled and launched dynamically at runtime based on the ILLIXR configuration used. ILLIXR also implements a Monado runtime translation Plugin . For a list of supported plugins and their details, see the ILLIXR Plugins page. For instructions for how to modify or write your own plugins, see the Modifying a Plugin and Writing Your Plugin pages. See the Plugin API documentation .","title":"Plugin"},{"location":"glossary/#configuration","text":"A file describing the key information required to launch an ILLIXR application. Configurations for ILLIXR are implemented as YAML files. Each configuration comprises an action , a profile , and a list of plugins as defined by our configuration specification Schema . Action (Previously Loader) : An action encapsulates a task for Runner . native : The default application launch configuration. Does not use our Monado runtime integration. Defined in ILLIXR/configs/native.yaml . native-lookup : Same as native , but using a ground truth lookup from a file for the pose instead of computing it. Defined in ILLIXR/configs/native-lookup.yaml . headless : Same as native , but using Xvfb to run without a graphical environment. Defined in ILLIXR/configs/headless.yaml . ci : Same as headless , but using Docker virtualization and debug-enabled compilation. Defined in ILLIXR/configs/ci.yaml . monado : Similar to native , but uses our Monado runtime integration. Defined in ILLIXR/configs/monado.yaml . clean : A meta-task that fetches all supported plugins and then cleans up builds across the entire ILLIXR project. Defined in ILLIXR/configs/clean.yaml and supported by ILLIXR/clean.sh . docs : A meta-task that generates and populates the documention subdirectories in the project. Profile : A profile captures the compilation mode used by Runner . opt : Sets Runner to compile the ILLIXR application and plugins with optimizations. dbg : Sets Runner to compile the ILLIXR application and plugins without optimizations, while enabling debug logic and debug logging. Schema : A schema captures the specification describing the allowable structure of a configuration file. Our schema is implemented using the json-schema specification . Defined in ILLIXR/runner/config_schema.yaml . For more details about the structure of a configuration, see the Building ILLIXR page .","title":"Config(uration)"},{"location":"glossary/#framebuffer","text":"A region of memory used to hold graphical information to be output to a display or graphics device. Frame : A single frame (image) to be output to a display at a certain instant of time based on the system's frame rate . Frame Rate : The interval period between complete (as defined by the output resolution) frame updates and refreshes. In many systems, the target frame rate is determined by a fixed vertical sync ( VSYNC ) period. Depth Buffer : A framebuffer representing the depth information of a 3D scene. Depth information is useful for applications such as graphics and SLAM . Eye Buffer : A framebuffer dedicated for display through a HMD lens to be perceived by a user's eye. For more information, see the Wikipedia article .","title":"Framebuffer"},{"location":"glossary/#swap-chain","text":"A set of virtual framebuffers to be output to a display. Only one framebuffer in a swap chain is displayed at a time, enabling the other virtual framebuffers to be concurrently modified in memory. For more information, see the Wikipedia article .","title":"Swap Chain"},{"location":"glossary/#compositor","text":"A window manager that establishes a framebuffer for each window of a graphical system. A compositor merges information across its windows to construct a unified framebuffer. For more information, see the Wikipedia article .","title":"Compositor"},{"location":"glossary/#head-mounted-display","text":"A display device worn on the head and face for use with VR and XR applications. Also known as a HMD . For more information, see the Wikipedia article .","title":"Head-mounted Display"},{"location":"glossary/#eye-tracking","text":"The process of measuring the eye movement of a user (who is possibly also wearing a HMD ). For more information, see the Wikipedia article .","title":"Eye Tracking"},{"location":"glossary/#event-stream","text":"A communication interface supporting writes, sychronous reads, and asynchronous reads. For synchronous reads, every value written to the stream is visible to consumers. For asynchronous reads, only the latest values written are guaranteed to be visible to consumers.","title":"Event Stream"},{"location":"glossary/#pose","text":"The combination of orientation and position of an object, used for computer vision and robotics applications. ILLIXR applications make use of poses to track the user's HMD within the virtual environment. Internally, ILLIXR has multiple classifications of poses which are used for various purposes. Slow Pose : A slow pose is a ... TODO Fast Pose : A fast pose is a ... TODO True Pose : A true pose is a ... TODO Depracated starting ILLIXR release v2.X.X . Pose Prediction : To improve the user's perception latency experience the time between, pose prediction uses history and current system information to pre-compute the user's next pose Pre-computing the next pose allows for components downstream from the pose output in the event stream dataflow graph to begin computation. Pose Prediction is implemented in the pose_prediction ILLIXR plugin . For more information, see the Wikipedia article .","title":"Pose"},{"location":"glossary/#ground-truth","text":"The most accurate source of measurement available for a data set. Typically, ground truth measurements are provided for the evaluation of sensor data where the sensor or other data source is not as accurate or reliable as the source for the ground truth. Ground Truth Poses : A collection of poses used to evaluate the accuracy of pose generation and prediction algorithms. Ground Truth Images : A collection of images used to evaluate the accuracy of visual processing algorithms, like SLAM and VIO . See the ILLIXR Plugins page for information about sensors implemented in ILLIXR.","title":"Ground Truth"},{"location":"glossary/#inertial-measurement-unit","text":"A device that reports its orientation in space and any forces applied it. Also known as an IMU . An IMU is implemented in the offline_imu_cam ILLIXR plugin . For more information, see the Wikipedia article .","title":"Inertial Measurement Unit"},{"location":"glossary/#simultaneous-localization-and-mapping","text":"The computational process of creating a map of an unknown environment, and finding one's location within that space. Also known as SLAM . For more information, see the Wikipedia article .","title":"Simultaneous Localization and Mapping"},{"location":"glossary/#visual-interial-odometry","text":"The process of computing a pose estimate from incoming visual information and measurements from the IMU . Also known as VIO . Often used in combination with SLAM techniques. See the Wikipedia article .","title":"Visual Interial Odometry"},{"location":"glossary/#asynchronous-reprojection","text":"The processing of rendered video for motion interpolation. Asynchronous reprojection improves the perception of the rendered video to the HMD when rendering misses it target frame rate . Asynchronous reprojection is implemented in the timewarpgl ILLIXR plugin . See the Wikipedia article .","title":"Asynchronous Reprojection"},{"location":"glossary/#distortion-correction","text":"The processing of visual anomalies in images where rectilinear features have been warped. For more information, see the Wikipedia artice .","title":"Distortion Correction"},{"location":"glossary/#chromatic-abberation-correction","text":"The processing of visual anomalies in images where colors are diffracted due to imperfect optics or other perturbing factors. For more information, see the Wikipedia article .","title":"Chromatic Abberation Correction"},{"location":"glossary/#components","text":"","title":"Components"},{"location":"glossary/#runner","text":"An ILLIXR tool responsible for preparing the environment, downloading required assets & code, compiling each plugin, and launching the ILLIXR application. The implementation resides in ILLIXR/runner/ , and can be launched with the appropriate environment setup via ILLIXR/runner.sh . Action (Previously Loader) : See Configuration .","title":"Runner"},{"location":"glossary/#spindle","text":"An ILLIXR component responsible for launching and managing plugin threads. The implementation resides in ILLIXR/runtime/ . See the Spindle API documentation .","title":"Spindle"},{"location":"glossary/#phonebook","text":"An ILLIXR service directory used to introspectively interface plugins and their data. The implementation resides in ILLIXR/runtime/ . See the Phonebook API documentation .","title":"Phonebook"},{"location":"glossary/#switchboard","text":"An ILLIXR event stream manager that maintains data pipelines between plugins. The implementation resides in ILLIXR/runtime/ . See the Switchboard API documentation .","title":"Switchboard"},{"location":"glossary/#technologies","text":"","title":"Technologies"},{"location":"glossary/#openxr","text":"An open standard for Augmented and Virtual Reality. ILLIXR components target the OpenXR standard and interact with the ILLIXR device via the Application Interface. For more information, visit the official site from the Khronos Group .","title":"OpenXR"},{"location":"glossary/#monado","text":"An open source, modular implementation of the OpenXR standard for GNU/Linux . See the ILLIXR Monado Overview and Monado Dataflow pages for details about our runtime integration using Monado. For more information, visit the official Monado development site .","title":"Monado"},{"location":"glossary/#godot","text":"An open source game development engine. ILLIXR applications targeting the OpenXR use Godot to access the engine's integration with the OpenXR standard via Monado . For more information, visit the official Godot site .","title":"Godot"},{"location":"glossary/#xvfb","text":"A virtual framebuffer for the X11 Window Sytem . ILLIXR uses Xvfb to enable running the graphical ILLIXR application without requiring the user to have a graphical environment configured at application launch. For more information, see the Xfvb man page .","title":"Xvfb"},{"location":"glossary/#docker","text":"A platform and containerization framework for deploying applications under virtualization. ILLIXR uses Docker to deploy and test code in a continuous integration and deployment pipeline. For more information, see the Docker overview and getting started page .","title":"Docker"},{"location":"glossary/#qemu-kvm","text":"An open source virtulization tool and machine emulator. See the instructions for running ILLIXR under Virtualization . For more information, see the official QEMU page .","title":"QEMU-KVM"},{"location":"glossary/#yaml","text":"A markup language and data serilization standard designed to be user friendly. We make use of the PyYAML and pyyaml-include libraries to implement our Configuration implementation. For more information, visit the official YAML page .","title":"YAML"},{"location":"glossary/#sqlite","text":"A SQL database engine implementation in C designed to be lightweight and easy to use. The ILLIXR project allows user to records application statistics to a local database for efficient processing. See the Logging and Metrics page for usage details. For more information, see the SQLite development site .","title":"SQLite"},{"location":"glossary/#vulkan","text":"A cross-platform graphics API that allows developers to efficiently target low-level hardware features. For more information, see the official Vulkan page from the Khronos Group .","title":"Vulkan"},{"location":"glossary/#opengl","text":"A cross-platform graphics API that allows developers to create graphics applications easily and portably. Also known as GL . GL Context : A data structure storing the state of an OpenGL application instance. Within a GL context resides framebuffer data. It is not thread safe to share contexts without appropriate synchronization. GLFW : An open source implementation of OpenGL. Supports Windows, MacOS and, Linux ( X11 and Wayland). See the GLFW development site . For more information, see the official OpenGL page from the Khronos Group .","title":"OpenGL"},{"location":"glossary/#ubuntu","text":"An open source GNU/Linux operating system and distribution. ILLIXR currently supports the Long Term Support (LTS) versions of Ubuntu: 18.04 LTS (Bionic) and 20.04 LTS (Focal). For more information, visit the official Ubuntu site .","title":"Ubuntu"},{"location":"illixr_plugins/","text":"ILLIXR plugins This page details the structure of ILLIXR's plugins and how they interact with each other. Default Plugins offline_imu_cam : Reads IMU data and images from files on disk, emulating a real sensor on the headset (feeds the application input measurements with timing similar to an actual IMU). Topic details: Publishes imu_cam_type on imu_cam topic. ground_truth_slam : Reads the ground truth from the same dataset as the offline_imu_cam plugin. Ground truth data can be compared against the measurements from offline_imu_cam for accuracy. Timing information is taken from the offline_imu_cam measurements/data. Topic details: Publishes pose_type on true_pose topic. Asynchronously reads imu_cam_type on imu_cam topic. kimera_vio : Runs Kimera-VIO ( upstream ) on the input, and outputs the headset's pose . In practice, the Kimera-VIO plugin publishes a fairly slow pose , so IMU integration and pose prediction is required to infer a fast pose . Topic details: Publishes pose_type on slow_pose topic. Publishes imu_integrator_input on imu_integrator_input topic. Synchronously reads / subscribes to imu_cam_type on imu_cam topic. gtsam_integrator : Integrates over all IMU samples since the last published SLAM pose to provide a fast pose every time a new IMU sample arrives using the GTSAM library ( upstream ). Topic details: Publishes imu_raw_type on imu_raw topic. Synchronously reads/subscribes to imu_cam_type on imu_cam topic. Asynchronously reads imu_integrator_input on imu_integrator_input topic. pose_prediction : Uses the latest IMU value to predict a pose for a future point in time. Implements the pose_prediction service (defined in common ), so poses can be served directly to other plugins. Topic details: Asynchronously reads pose_type on slow_pose topic, but it is only used as a fallback. Asynchronously reads imu_raw on imu_raw topic. Asynchronously reads pose_type on true_pose topic, but it is only used if the client asks for the true pose. Asynchronously reads time_type on vsync_estimate topic. This tells pose_predict what time to estimate for. gldemo : Renders a static scene (into left and right eye buffers ) given the pose from pose_prediction . Topic details: Calls pose_prediction . Publishes rendered_frame on eyebuffer topic. Asynchronously reads time_type on vsync_estimate topic. timewarp_gl : Asynchronous reprojection of the eye buffers . The timewarp ends just after vsync , so it can deduce when the next vsync will be. Topic details: Calls pose_prediction . Asynchronously reads rendered_frame on eyebuffer topic. Publishes time_type on vsync_estimate topic. Publishes hologram_input on hologram_in topic. Publishes texture_pose on texture_pose topic if ILLIXR_OFFLOAD_ENABLE is set in the env. debugview : Renders incoming frames from the graphics pipeline for debugging live executions of the application. Topic details: Calls pose_prediction . Asynchronously reads fast_pose on imu_raw topic. ( IMU biases are unused). Asynchronously reads slow_pose on slow_pose topic. Synchronously reads imu_cam on imu_cam topic. audio_pipeline : Launches a thread for binaural recording and one for binaural playback. Audio output is not yet routed to the system's speakers or microphone, but the plugin's compute workload is still representative of a real system. By default this plugin is enabled (see native configuration ). Topic details: Calls pose_prediction . Below this point, we will use Switchboard terminology. Read the API documentation on Switchboard for more information. In the above figure, rectangles are plugins. Solid arrows from plugins to topics represent publishing. Solid arrows from topics to plugins represent synchronous reading. Some action is taken for every event which gets published on the topic. Dashed arrows from topics to plugins represent asynchronous reading. Plugin readers only need the latest event on their topic. Imagine the topic as a trough filling with events from its publisher. Synchronous readers (AKA subscribers) drain the trough, while asynchronous readers just skim fresh events off the top of the trough. See Writing Your Plugin to extend ILLIXR. Other Supported Plugins ILLIXR supports additional plugins to replace some of the default plugins. hologram : Adapts the eyebuffer for use on a holographic display. By default, this plugin is disabled, since an NVIDIA GPU is currently required. Topic details: Asynchronously reads hologram_input on hologram_in topic. Hologram is too slow to run for every input, so the plugin implements an asynchronous reader which can drop inputs. open_vins : An alternate SLAM ( upstream ) implementation that uses a MSCKF (Multi-State Constrained Kalman Filter) to determine poses via camera/ IMU . Topic details: Same interface as Kimera-VIO . rk4_integrator : Integrates over all IMU samples since the last published SLAM pose to provide a fast pose every time a new IMU sample arrives using RK4 integration. Topic details: Same interface as gtsam_integrator . pose_lookup : Implements the pose_predict service, but uses ground truth from the dataset. The plugin peeks \"into the future\" to determine what the exact pose will be at a certain time. Topic details: Asynchronously reads time_type on vsync_estimate topic. This tells pose_lookup what time to lookup. offload_data : Writes frames and poses output from the asynchronous reprojection plugin to disk for analysis. Topic details: Synchronously reads texture_pose on texture_pose topic. zed : Reads images and IMU measurements from the ZED Mini . Unlike offline_imu_cam , zed additionally has RGB and depth data. Note that this plugin implements two threads: one for the camera, and one for the IMU. Topic details: Publishes imu_cam_type on imu_cam topic. Publishes rgb_depth_type on rgb_depth topic. realsense : Reads images and IMU measurements from the Intel Realsense . Topic details: Same interface as zed . See Building ILLIXR for more information on adding plugins to a config file.","title":"ILLIXR Plugins (Version 2)"},{"location":"illixr_plugins/#illixr-plugins","text":"This page details the structure of ILLIXR's plugins and how they interact with each other.","title":"ILLIXR plugins"},{"location":"illixr_plugins/#default-plugins","text":"offline_imu_cam : Reads IMU data and images from files on disk, emulating a real sensor on the headset (feeds the application input measurements with timing similar to an actual IMU). Topic details: Publishes imu_cam_type on imu_cam topic. ground_truth_slam : Reads the ground truth from the same dataset as the offline_imu_cam plugin. Ground truth data can be compared against the measurements from offline_imu_cam for accuracy. Timing information is taken from the offline_imu_cam measurements/data. Topic details: Publishes pose_type on true_pose topic. Asynchronously reads imu_cam_type on imu_cam topic. kimera_vio : Runs Kimera-VIO ( upstream ) on the input, and outputs the headset's pose . In practice, the Kimera-VIO plugin publishes a fairly slow pose , so IMU integration and pose prediction is required to infer a fast pose . Topic details: Publishes pose_type on slow_pose topic. Publishes imu_integrator_input on imu_integrator_input topic. Synchronously reads / subscribes to imu_cam_type on imu_cam topic. gtsam_integrator : Integrates over all IMU samples since the last published SLAM pose to provide a fast pose every time a new IMU sample arrives using the GTSAM library ( upstream ). Topic details: Publishes imu_raw_type on imu_raw topic. Synchronously reads/subscribes to imu_cam_type on imu_cam topic. Asynchronously reads imu_integrator_input on imu_integrator_input topic. pose_prediction : Uses the latest IMU value to predict a pose for a future point in time. Implements the pose_prediction service (defined in common ), so poses can be served directly to other plugins. Topic details: Asynchronously reads pose_type on slow_pose topic, but it is only used as a fallback. Asynchronously reads imu_raw on imu_raw topic. Asynchronously reads pose_type on true_pose topic, but it is only used if the client asks for the true pose. Asynchronously reads time_type on vsync_estimate topic. This tells pose_predict what time to estimate for. gldemo : Renders a static scene (into left and right eye buffers ) given the pose from pose_prediction . Topic details: Calls pose_prediction . Publishes rendered_frame on eyebuffer topic. Asynchronously reads time_type on vsync_estimate topic. timewarp_gl : Asynchronous reprojection of the eye buffers . The timewarp ends just after vsync , so it can deduce when the next vsync will be. Topic details: Calls pose_prediction . Asynchronously reads rendered_frame on eyebuffer topic. Publishes time_type on vsync_estimate topic. Publishes hologram_input on hologram_in topic. Publishes texture_pose on texture_pose topic if ILLIXR_OFFLOAD_ENABLE is set in the env. debugview : Renders incoming frames from the graphics pipeline for debugging live executions of the application. Topic details: Calls pose_prediction . Asynchronously reads fast_pose on imu_raw topic. ( IMU biases are unused). Asynchronously reads slow_pose on slow_pose topic. Synchronously reads imu_cam on imu_cam topic. audio_pipeline : Launches a thread for binaural recording and one for binaural playback. Audio output is not yet routed to the system's speakers or microphone, but the plugin's compute workload is still representative of a real system. By default this plugin is enabled (see native configuration ). Topic details: Calls pose_prediction . Below this point, we will use Switchboard terminology. Read the API documentation on Switchboard for more information. In the above figure, rectangles are plugins. Solid arrows from plugins to topics represent publishing. Solid arrows from topics to plugins represent synchronous reading. Some action is taken for every event which gets published on the topic. Dashed arrows from topics to plugins represent asynchronous reading. Plugin readers only need the latest event on their topic. Imagine the topic as a trough filling with events from its publisher. Synchronous readers (AKA subscribers) drain the trough, while asynchronous readers just skim fresh events off the top of the trough. See Writing Your Plugin to extend ILLIXR.","title":"Default Plugins"},{"location":"illixr_plugins/#other-supported-plugins","text":"ILLIXR supports additional plugins to replace some of the default plugins. hologram : Adapts the eyebuffer for use on a holographic display. By default, this plugin is disabled, since an NVIDIA GPU is currently required. Topic details: Asynchronously reads hologram_input on hologram_in topic. Hologram is too slow to run for every input, so the plugin implements an asynchronous reader which can drop inputs. open_vins : An alternate SLAM ( upstream ) implementation that uses a MSCKF (Multi-State Constrained Kalman Filter) to determine poses via camera/ IMU . Topic details: Same interface as Kimera-VIO . rk4_integrator : Integrates over all IMU samples since the last published SLAM pose to provide a fast pose every time a new IMU sample arrives using RK4 integration. Topic details: Same interface as gtsam_integrator . pose_lookup : Implements the pose_predict service, but uses ground truth from the dataset. The plugin peeks \"into the future\" to determine what the exact pose will be at a certain time. Topic details: Asynchronously reads time_type on vsync_estimate topic. This tells pose_lookup what time to lookup. offload_data : Writes frames and poses output from the asynchronous reprojection plugin to disk for analysis. Topic details: Synchronously reads texture_pose on texture_pose topic. zed : Reads images and IMU measurements from the ZED Mini . Unlike offline_imu_cam , zed additionally has RGB and depth data. Note that this plugin implements two threads: one for the camera, and one for the IMU. Topic details: Publishes imu_cam_type on imu_cam topic. Publishes rgb_depth_type on rgb_depth topic. realsense : Reads images and IMU measurements from the Intel Realsense . Topic details: Same interface as zed . See Building ILLIXR for more information on adding plugins to a config file.","title":"Other Supported Plugins"},{"location":"logging_and_metrics/","text":"Logging and Metrics The ILLIXR project supports several ways for an ILLIXR application to log and report details about its execution. Logging ILLIXR implements a modular logging system that enables users to capture and record key statistics in real-time. record_logger : The base class describing ILLIXR's logging interface. noop_logger : Implements a trivially empty implementation of record_logger . Can be used for debugging or performance if runtime statistics are not needed. sqlite_record_logger : Extends the record_logger to store records in a local SQLite database . Metrics ILLIXR allows users to generate higher order statistics from logged results called Metrics . TODO","title":"Logging and Metrics (Version 2)"},{"location":"logging_and_metrics/#logging-and-metrics","text":"The ILLIXR project supports several ways for an ILLIXR application to log and report details about its execution.","title":"Logging and Metrics"},{"location":"logging_and_metrics/#logging","text":"ILLIXR implements a modular logging system that enables users to capture and record key statistics in real-time. record_logger : The base class describing ILLIXR's logging interface. noop_logger : Implements a trivially empty implementation of record_logger . Can be used for debugging or performance if runtime statistics are not needed. sqlite_record_logger : Extends the record_logger to store records in a local SQLite database .","title":"Logging"},{"location":"logging_and_metrics/#metrics","text":"ILLIXR allows users to generate higher order statistics from logged results called Metrics . TODO","title":"Metrics"},{"location":"modifying_a_plugin/","text":"Modifying a plugin Tutorial This is how you can modify an existing ILLIXR plugin Clone the repository for the component you want to modify. For example: git clone https://github.com/ILLIXR/audio_pipeline.git Modify the config file like this: Original Config plugin_group: - path: timewarp_gl/ - name: audio path: git_repo: https://github.com/ILLIXR/audio_pipeline.git version: 3433bb452b2ec661c9d3ef65d9cf3a2805e94cdc New Config plugin_group: - path: timewarp_gl/ - name: /PATH/TO/LOCAL/AUDIO-PLUGIN See the instructions on Building ILLIXR to learn how to run ILLIXR. To push the modification to upstream ILLIXR, push up the changes to the plugin's repository and modify the original config with the commit version updated. Then create a PR on the main ILLIXR repository.","title":"Modifying a Plugin (Version 2)"},{"location":"modifying_a_plugin/#modifying-a-plugin","text":"","title":"Modifying a plugin"},{"location":"modifying_a_plugin/#tutorial","text":"This is how you can modify an existing ILLIXR plugin Clone the repository for the component you want to modify. For example: git clone https://github.com/ILLIXR/audio_pipeline.git Modify the config file like this: Original Config plugin_group: - path: timewarp_gl/ - name: audio path: git_repo: https://github.com/ILLIXR/audio_pipeline.git version: 3433bb452b2ec661c9d3ef65d9cf3a2805e94cdc New Config plugin_group: - path: timewarp_gl/ - name: /PATH/TO/LOCAL/AUDIO-PLUGIN See the instructions on Building ILLIXR to learn how to run ILLIXR. To push the modification to upstream ILLIXR, push up the changes to the plugin's repository and modify the original config with the commit version updated. Then create a PR on the main ILLIXR repository.","title":"Tutorial"},{"location":"monado_illixr_runtime_overview/","text":"Monado Integration Overview ILLIXR's Plugins provide XR services, and the Runtime ties them together. However, we don't want to force developers to write their whole application specifically for ILLIXR. As such, we want to implement a common interface XR runtimes, such as OpenXR , so one application can work on several runtimes (including ours). In order to support OpenXR, we modified Monado , an existing, open-source implementation of the standard. When running ILLIXR without Monado, the ILLIXR runtime is the entry-point. Phonebook and switchboard are initialized and plugins are loaded, among which is the gldemo app. When running from Monado, however, as mandated by OpenXR specifications, the application is the entry point. As a result, the ILLIXR runtime system is loaded at a later point as a shared library. This page documents the changes to the ILLIXR runtime when an OpenXR application is used. OpenXR Application Launch As specified by OpenXR , the OpenXR application initializes the OpenXR runtime by reading a configuration JSON file pointed to by an environment variable and loads the OpenXR runtime, which is Monado in this case, as a shared library into its address space. Consult the OpenXR specifications and the OpenXR-SDK from Khronos Group for more details. Monado Device Probe and ILLIXR Initialization During initialization, Monado asks all drivers to probe for and initialize HMDs and controllers, internally known as xdev s. Our ILLIXR driver will always respond to Monado with one discovered HMD that will be used to capture OpenXR queries and events from Monado's state tracker. The driver obtains the path to the ILLIXR runtime .so file and a list of plugins from environment variables. After probing is finished, the application will start to create an OpenXR session. At some point in this process, the application will send its rendering context to the runtime, which we capture and send to the ILLIXR driver. At this moment, all necessary data is ready and ILLIXR will be launched. ILLIXR Runtime Launch When used with Monado , the ILLIXR Runtime is compiled into a shared library instead of an executable. The library exports its two major functionalities: initializing Switchboard and Phonebook , and loading Plugins . The driver starts to load the runtime by loading the shared library into the current (application's) address space and calls the Switchboard and Phonebook initialization. Then, it calls the plugin loading for each ILLIXR plugin (except gldemo , which is replaced by the OpenXR app). Finally, it calls a special plugin loading which takes a function address instead of a file path to load a Translation Plugin into ILLIXR as the application. If the plugin implements a long running computation, it may block the main ILLIXR thread which drives the entire application. To remedy this, a plugin should implement long running processing in its own thread. This way, the driver will be able to reacquire control and return to Monado and the application efficiently. Translation Plugin When the application and all ILLIXR plugins are up and running, the translation plugin handles the connection between Monado and ILLIXR. It might be confusing to see that this plugin is part of the ILLIXR driver which is part of Monado while at the same time also part of ILLIXR as a plugin. However, Monado and ILLIXR are running in different threads in the same address space. The translation plugin is the interface of these two parallel systems. The translation plugin handles two types of events at the moment: pose requests and frame submissions. From the view of Monado, the translation plugin is the destination of all requests: from the application, to Monado's state trackers, to the xdev interface who is responsible for servicing the request. From the view of ILLIXR, the translation plugin behaves the same as the gldemo application : reading pose and submitting frames. For implementation details regarding the representation of poses and frames in Monado and in ILLIXR, please see ILLIXR's Monado Integration Dataflow .","title":"Monado Overview (Version 2)"},{"location":"monado_illixr_runtime_overview/#monado-integration-overview","text":"ILLIXR's Plugins provide XR services, and the Runtime ties them together. However, we don't want to force developers to write their whole application specifically for ILLIXR. As such, we want to implement a common interface XR runtimes, such as OpenXR , so one application can work on several runtimes (including ours). In order to support OpenXR, we modified Monado , an existing, open-source implementation of the standard. When running ILLIXR without Monado, the ILLIXR runtime is the entry-point. Phonebook and switchboard are initialized and plugins are loaded, among which is the gldemo app. When running from Monado, however, as mandated by OpenXR specifications, the application is the entry point. As a result, the ILLIXR runtime system is loaded at a later point as a shared library. This page documents the changes to the ILLIXR runtime when an OpenXR application is used.","title":"Monado Integration Overview"},{"location":"monado_illixr_runtime_overview/#openxr-application-launch","text":"As specified by OpenXR , the OpenXR application initializes the OpenXR runtime by reading a configuration JSON file pointed to by an environment variable and loads the OpenXR runtime, which is Monado in this case, as a shared library into its address space. Consult the OpenXR specifications and the OpenXR-SDK from Khronos Group for more details.","title":"OpenXR Application Launch"},{"location":"monado_illixr_runtime_overview/#monado-device-probe-and-illixr-initialization","text":"During initialization, Monado asks all drivers to probe for and initialize HMDs and controllers, internally known as xdev s. Our ILLIXR driver will always respond to Monado with one discovered HMD that will be used to capture OpenXR queries and events from Monado's state tracker. The driver obtains the path to the ILLIXR runtime .so file and a list of plugins from environment variables. After probing is finished, the application will start to create an OpenXR session. At some point in this process, the application will send its rendering context to the runtime, which we capture and send to the ILLIXR driver. At this moment, all necessary data is ready and ILLIXR will be launched.","title":"Monado Device Probe and ILLIXR Initialization"},{"location":"monado_illixr_runtime_overview/#illixr-runtime-launch","text":"When used with Monado , the ILLIXR Runtime is compiled into a shared library instead of an executable. The library exports its two major functionalities: initializing Switchboard and Phonebook , and loading Plugins . The driver starts to load the runtime by loading the shared library into the current (application's) address space and calls the Switchboard and Phonebook initialization. Then, it calls the plugin loading for each ILLIXR plugin (except gldemo , which is replaced by the OpenXR app). Finally, it calls a special plugin loading which takes a function address instead of a file path to load a Translation Plugin into ILLIXR as the application. If the plugin implements a long running computation, it may block the main ILLIXR thread which drives the entire application. To remedy this, a plugin should implement long running processing in its own thread. This way, the driver will be able to reacquire control and return to Monado and the application efficiently.","title":"ILLIXR Runtime Launch"},{"location":"monado_illixr_runtime_overview/#translation-plugin","text":"When the application and all ILLIXR plugins are up and running, the translation plugin handles the connection between Monado and ILLIXR. It might be confusing to see that this plugin is part of the ILLIXR driver which is part of Monado while at the same time also part of ILLIXR as a plugin. However, Monado and ILLIXR are running in different threads in the same address space. The translation plugin is the interface of these two parallel systems. The translation plugin handles two types of events at the moment: pose requests and frame submissions. From the view of Monado, the translation plugin is the destination of all requests: from the application, to Monado's state trackers, to the xdev interface who is responsible for servicing the request. From the view of ILLIXR, the translation plugin behaves the same as the gldemo application : reading pose and submitting frames. For implementation details regarding the representation of poses and frames in Monado and in ILLIXR, please see ILLIXR's Monado Integration Dataflow .","title":"Translation Plugin"},{"location":"monado_integration_dataflow/","text":"Monado Integration Dataflow The dataflow for the ILLIXR Monado integration comprises two steps: 1. getting pose data from ILLIXR, and 1. sending a user rendered frame back to ILLIXR. In Monado, ILLIXR is recognized as an HMD for Monado, while in ILLIXR, Monado looks like a user application (such as gldemo ). After ILLIXR is initialized from Monado, and Monado is registered as a plugin for ILLIXR, the most recent pose information can be easily obtained via Switchboard . The compositor side of Monado integration with ILLIXR is implemented more subtly. The original Monado compositor primarily performs distortion correction and aberration correction in a Vulkan back-end compositor. The compositor also has two client compositors (one for OpenGL applications and another for Vulkan applications) which pass frame data to the back-end compositor. ILLIXR integration intercepts the frame at GL client compositor and sends it to Switchboard of ILLIXR, which is then used by timewarp_gl component . To get an OpenGL frame and use it without copying pixels, ILLIXR needs to get the user application GL context. This is done at OpenXR session creation time, where ILLIXR is initialized. Note that, logically, ILLIXR is initialized during OpenXR instance creation, or is otherwise running in the background all the time. Currently, ILLIXR is initialized at session creation time, since ILLIXR only supports single OpenXR session, and requires a user application GL context upon initialization, The current ILLIXR integration for Monado is a temporary solution and has some drawbacks caused by the concurrent and continued development from both the Monado and ILLIXR projects. The integration: Does not use the pose that user application declares to use at rendering (using the OpenXR specification). This is due to incongruencies with Monado's internal interfaces and representations. The pose difference used by timewarp is computed using the most recent query for a pose update. Cannot submit frame data with a depth buffer. Cannot have poses that make use of OpenXR Spaces . Raw pose data is instead retrieved from the application's SLAM algorithms. Does not support controller action. Only supports GL user-space applications. User-space applications cannot acquire more than one swap chain buffer for each eye during the the processing of a frame. Must initialize ILLIXR during the session initialization.","title":"Monado Dataflow (Version 2)"},{"location":"monado_integration_dataflow/#monado-integration-dataflow","text":"The dataflow for the ILLIXR Monado integration comprises two steps: 1. getting pose data from ILLIXR, and 1. sending a user rendered frame back to ILLIXR. In Monado, ILLIXR is recognized as an HMD for Monado, while in ILLIXR, Monado looks like a user application (such as gldemo ). After ILLIXR is initialized from Monado, and Monado is registered as a plugin for ILLIXR, the most recent pose information can be easily obtained via Switchboard . The compositor side of Monado integration with ILLIXR is implemented more subtly. The original Monado compositor primarily performs distortion correction and aberration correction in a Vulkan back-end compositor. The compositor also has two client compositors (one for OpenGL applications and another for Vulkan applications) which pass frame data to the back-end compositor. ILLIXR integration intercepts the frame at GL client compositor and sends it to Switchboard of ILLIXR, which is then used by timewarp_gl component . To get an OpenGL frame and use it without copying pixels, ILLIXR needs to get the user application GL context. This is done at OpenXR session creation time, where ILLIXR is initialized. Note that, logically, ILLIXR is initialized during OpenXR instance creation, or is otherwise running in the background all the time. Currently, ILLIXR is initialized at session creation time, since ILLIXR only supports single OpenXR session, and requires a user application GL context upon initialization, The current ILLIXR integration for Monado is a temporary solution and has some drawbacks caused by the concurrent and continued development from both the Monado and ILLIXR projects. The integration: Does not use the pose that user application declares to use at rendering (using the OpenXR specification). This is due to incongruencies with Monado's internal interfaces and representations. The pose difference used by timewarp is computed using the most recent query for a pose update. Cannot submit frame data with a depth buffer. Cannot have poses that make use of OpenXR Spaces . Raw pose data is instead retrieved from the application's SLAM algorithms. Does not support controller action. Only supports GL user-space applications. User-space applications cannot acquire more than one swap chain buffer for each eye during the the processing of a frame. Must initialize ILLIXR during the session initialization.","title":"Monado Integration Dataflow"},{"location":"virtualization/","text":"Setting up ILLIXR in QEMU Build QEMU Run ILLIXR/install_deps.sh and select yes when asked to install QEMU . This will build QEMU and install it to /opt/ILLIXR . Why build QEMU from source? The version of QEMU available through package managers doesn't always ship with all the options we need to run ILLIXR, so building QEMU from source is the best option. This qemu installation will not conflict with existing qemu installs on your system. Setup Ubuntu in the VM Run ILLIXR/qemu/run.sh to download Ubuntu 18.04 , create a virtual hard drive ( illixr.qcow2 ), and launch qemu from /opt/ILLIXR . Your VM image will be created at ILLIXR/qemu/illixr.qcow2 . Ubuntu will be downloaded and saved at ILLIXR/qemu/ubuntu-18.04.5-desktop-amd64.iso . You will be prompted to install Ubuntu; follow the instructions and install Ubuntu to the virtual hard drive. Choose the \"erase all\" option and confirm: Pick any account name and password you like. Once Ubuntu is installed you will be asked to reboot. Close qemu and then run ./run.sh again to boot into your brand new Ubuntu install! Booting the VM To launch the VM from now on, just use ILLIXR/qemu/run.sh . This will boot from the Ubuntu image we created earlier ( illixr.qcow2 ). Once Ubuntu is installed, it is safe to delete ubuntu-18.04.5-desktop-amd64.iso . Setting up the VM Once inside the VM, set up and run ILLIXR as found on the Getting Started page . Uninstalling To delete your local VM, just delete ILLIXR/qemu/illixr.qcow2 . ILLIXR/qemu/ubuntu-18.04.5-desktop-amd64.iso can be deleted anytime you want after Ubuntu is installed to your VM. If you've deleted illixr.qcow2 , you can run run.sh to recreate it and reinstall everything.","title":"ILLIXR under Virtualization (Version 2)"},{"location":"virtualization/#setting-up-illixr-in-qemu","text":"","title":"Setting up ILLIXR in QEMU"},{"location":"virtualization/#build-qemu","text":"Run ILLIXR/install_deps.sh and select yes when asked to install QEMU . This will build QEMU and install it to /opt/ILLIXR .","title":"Build QEMU"},{"location":"virtualization/#why-build-qemu-from-source","text":"The version of QEMU available through package managers doesn't always ship with all the options we need to run ILLIXR, so building QEMU from source is the best option. This qemu installation will not conflict with existing qemu installs on your system.","title":"Why build QEMU from source?"},{"location":"virtualization/#setup-ubuntu-in-the-vm","text":"Run ILLIXR/qemu/run.sh to download Ubuntu 18.04 , create a virtual hard drive ( illixr.qcow2 ), and launch qemu from /opt/ILLIXR . Your VM image will be created at ILLIXR/qemu/illixr.qcow2 . Ubuntu will be downloaded and saved at ILLIXR/qemu/ubuntu-18.04.5-desktop-amd64.iso . You will be prompted to install Ubuntu; follow the instructions and install Ubuntu to the virtual hard drive. Choose the \"erase all\" option and confirm: Pick any account name and password you like. Once Ubuntu is installed you will be asked to reboot. Close qemu and then run ./run.sh again to boot into your brand new Ubuntu install!","title":"Setup Ubuntu in the VM"},{"location":"virtualization/#booting-the-vm","text":"To launch the VM from now on, just use ILLIXR/qemu/run.sh . This will boot from the Ubuntu image we created earlier ( illixr.qcow2 ). Once Ubuntu is installed, it is safe to delete ubuntu-18.04.5-desktop-amd64.iso .","title":"Booting the VM"},{"location":"virtualization/#setting-up-the-vm","text":"Once inside the VM, set up and run ILLIXR as found on the Getting Started page .","title":"Setting up the VM"},{"location":"virtualization/#uninstalling","text":"To delete your local VM, just delete ILLIXR/qemu/illixr.qcow2 . ILLIXR/qemu/ubuntu-18.04.5-desktop-amd64.iso can be deleted anytime you want after Ubuntu is installed to your VM. If you've deleted illixr.qcow2 , you can run run.sh to recreate it and reinstall everything.","title":"Uninstalling"},{"location":"writing_your_plugin/","text":"Writing Your Plugin Adding a New Plugin (Common Case) In the common case, you only need to define a Makefile with the line include common/common.mk and symlink common ( ln -s ../common common ). The included recipe file provides the necessary targets and uses the compiler $(CXX) , which is defined based on the OS and environment variables. The included Makefile : Compiles plugin.cpp and any other *.cpp files into the plugin. Will invoke a recompile of the target any time any *.hpp or *.cpp file changes. Compiles with C++17. You can change this in your plugin by defining STDCXX = ... before the include . This change will not affect other plugins; just yours. Accepts specifying libraries by appending to LDFLAGS and CFLAGS . For example: LDFLAGS := $(LDFLAGS) $(shell pkg-config --ldflags eigen3) CFLAGS := $(CFLAGS) $(shell pkg-config --cflags eigen3) See the source for the other flags and variables that you can set. Finally, place the path of your plugin directory in the plugin_group list for the configuration you would like to run (e.g. ILLIXR/configs/native.yaml ). Adding a New Plugin (General Case) Each plugin can have a completely independent build system, as long as: It defines a Makefile with targets for plugin.dbg.so , plugin.opt.so , and clean . Inside this Makefile , one can defer to another build system. Its compiler maintains ABI compatibility with the compilers used in every other plugin. Using the same version of Clang or GCC on the same architecture is sufficient for this. Its path is in the plugin_group list for the configuration you would like to run (e.g. ILLIXR/configs/native.yaml ). Tutorial You can extend ILLIXR for your own purposes. To add your own functionality via the plugin interface: Create a new directory anywhere for your new plugin and set it up for ILLIXR. We recommend you also push this plugin to a git repository on Github/Gitlab if you want it as a part of upstream ILLIXR in the future. Create a Makefile with the following contents. See Building ILLIXR for more details and alternative setups. include common.mk You must decide if your plugin should inherit the standardized threadloop or plugin . If your plugin just needs to run one computation repeatedly, then your plugin class should extend threadloop . If you need custom concurrency (more complicated than a loop), triggered concurrency (by events fired in other plugins), or no concurrency then your plugin class should extend plugin . If you spin your own threads, they must wait for pb->lookup_impl<Stoplight>()->wait_for_ready() the first time they run. This allows the start of all threads in ILLIXR to be synchronized. They must be joined-or-disowned at-or-before plugin::stop() . This allows ILLIXR to shutdown cleanly. Write a file called plugin.cpp with this body, replacing every instance of plugin_name : #include \"common/phonebook.hpp\" #include \"common/plugin.hpp\" #include \"common/threadloop.hpp\" using namespace ILLIXR; /// Inherit from `plugin` if you don't need the threadloop class plugin_name : public threadloop { public: plugin_name(std::string name_, phonebook* pb_) : threadloop{name_, pb_} { } virtual void start() override { } virtual ~plugin_name() override { } }; // This line makes the plugin importable by Spindle PLUGIN_MAIN(plugin_name); At this point, you should be able to build your plugin with ILLIXR. Move to the ILLIXR repo and update configs/native.yaml . If the new plugin is the same type as one of the other components you will need to remove that component from the config before running the new component. For example, if the new component is a SLAM then the old SLAM needs to be removed from the config. See Building ILLIXR for more details on the config file. plugin_groups: - !include \"rt_slam_plugins.yaml\" - !include \"core_plugins.yaml\" - plugin_group: - path: /PATH/TO/NEW/PLUGIN - path: ground_truth_slam/ - path: gldemo/ - path: debugview/ data: subpath: mav0 relative_to: archive_path: download_url: 'http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room1/V1_02_medium/V1_02_medium.zip' demo_data: demo_data/ loader: name: native # command: gdb -q --args %a profile: opt Finally, run ILLIXR with your new plugin with the following command: ./runner.sh configs/native.yaml This is all that is required to be a plugin which can be loaded by Spindle in the ILLIXR runtime. Reading and writing from Phonebook and Switchboard is optional, but nearly every plugin does it. See default_plugins.md for more details. First, we can query the phonebook to get various services including switchboard . Then we query switchboard for event-streams (topics). We will read topic1 , write to topic2 , and schedule computation on topic 3 . See the API documentation for phonebook and switchboard for more details. #include \"common/phonebook.hpp\" #include \"common/plugin.hpp\" #include \"common/threadloop.hpp\" /* When datatypes have to be common across plugins * (e.g. a phonebook service or switchboard topic), * they are defined in this header, * which is accessible to all plugins. */ #include \"common/data_format.hpp\" class plugin_name : public threadloop { public: /* After the constructor, C++ permits a list of member-constructors. * We use uniform initialization (curly-braces) [1] instead of parens to * avoid ambiguity [2]. * We put the comma at the start of the line, so that lines can be copied around * or deleted freely (except for the first). * * [1]: https://en.wikipedia.org/wiki/C%2B%2B11#Uniform_initialization * [2]: https://en.wikipedia.org/wiki/Most_vexing_parse */ plugin_name(std::string name_, phonebook* pb_) : threadloop{name_, pb_} /// Find the switchboard in phonebook , sb{pb->lookup_impl<switchboard>()} /// Create a handle to a topic in switchboard for subscribing , topic1{sb->get_reader<topic1_type>(\"topic1\")} /// Create a handle to a topic in switchboard for publishing , topic2{sb->get_writer<topic2_type>(\"topic2\")} { /// Read topic 1 switchboard::ptr<const topic1_type> event1 = topic1.get_ro(); /// Write to topic 2 topic2.put( topic2.allocate<topic2_type>( arg_1, // topic2_type::topic2_type() arg_type_1 ..., // ... arg_k // topic2_type::topic2_type() arg_type_k ) ); /// Read topic 3 synchronously sb->schedule<topic3_type>( get_name(), \"topic3\", [&](switchboard::ptr<const topic3_type> event3, std::size_t) { /* This is a [lambda expression][1] * * [1]: https://en.cppreference.com/w/cpp/language/lambda */ std::cout << \"Got a new event on topic3: \" << event3 << std::endl; callback(event3); } ); } virtual void _p_one_iteration override() { std::cout << \"Running\" << std::endl; auto target = std::chrono::system_clock::now() + std::chrono::milliseconds{10}; reliable_sleep(target); } private: const std::shared_ptr<switchboard> sb; switchboard::reader<topic1_type> topic1; switchboard::writer<topic2> topic2; }; /// This line makes the plugin importable by Spindle PLUGIN_MAIN(plugin_name);","title":"Writing your Plugin (Version 2)"},{"location":"writing_your_plugin/#writing-your-plugin","text":"","title":"Writing Your Plugin"},{"location":"writing_your_plugin/#adding-a-new-plugin-common-case","text":"In the common case, you only need to define a Makefile with the line include common/common.mk and symlink common ( ln -s ../common common ). The included recipe file provides the necessary targets and uses the compiler $(CXX) , which is defined based on the OS and environment variables. The included Makefile : Compiles plugin.cpp and any other *.cpp files into the plugin. Will invoke a recompile of the target any time any *.hpp or *.cpp file changes. Compiles with C++17. You can change this in your plugin by defining STDCXX = ... before the include . This change will not affect other plugins; just yours. Accepts specifying libraries by appending to LDFLAGS and CFLAGS . For example: LDFLAGS := $(LDFLAGS) $(shell pkg-config --ldflags eigen3) CFLAGS := $(CFLAGS) $(shell pkg-config --cflags eigen3) See the source for the other flags and variables that you can set. Finally, place the path of your plugin directory in the plugin_group list for the configuration you would like to run (e.g. ILLIXR/configs/native.yaml ).","title":"Adding a New Plugin (Common Case)"},{"location":"writing_your_plugin/#adding-a-new-plugin-general-case","text":"Each plugin can have a completely independent build system, as long as: It defines a Makefile with targets for plugin.dbg.so , plugin.opt.so , and clean . Inside this Makefile , one can defer to another build system. Its compiler maintains ABI compatibility with the compilers used in every other plugin. Using the same version of Clang or GCC on the same architecture is sufficient for this. Its path is in the plugin_group list for the configuration you would like to run (e.g. ILLIXR/configs/native.yaml ).","title":"Adding a New Plugin (General Case)"},{"location":"writing_your_plugin/#tutorial","text":"You can extend ILLIXR for your own purposes. To add your own functionality via the plugin interface: Create a new directory anywhere for your new plugin and set it up for ILLIXR. We recommend you also push this plugin to a git repository on Github/Gitlab if you want it as a part of upstream ILLIXR in the future. Create a Makefile with the following contents. See Building ILLIXR for more details and alternative setups. include common.mk You must decide if your plugin should inherit the standardized threadloop or plugin . If your plugin just needs to run one computation repeatedly, then your plugin class should extend threadloop . If you need custom concurrency (more complicated than a loop), triggered concurrency (by events fired in other plugins), or no concurrency then your plugin class should extend plugin . If you spin your own threads, they must wait for pb->lookup_impl<Stoplight>()->wait_for_ready() the first time they run. This allows the start of all threads in ILLIXR to be synchronized. They must be joined-or-disowned at-or-before plugin::stop() . This allows ILLIXR to shutdown cleanly. Write a file called plugin.cpp with this body, replacing every instance of plugin_name : #include \"common/phonebook.hpp\" #include \"common/plugin.hpp\" #include \"common/threadloop.hpp\" using namespace ILLIXR; /// Inherit from `plugin` if you don't need the threadloop class plugin_name : public threadloop { public: plugin_name(std::string name_, phonebook* pb_) : threadloop{name_, pb_} { } virtual void start() override { } virtual ~plugin_name() override { } }; // This line makes the plugin importable by Spindle PLUGIN_MAIN(plugin_name); At this point, you should be able to build your plugin with ILLIXR. Move to the ILLIXR repo and update configs/native.yaml . If the new plugin is the same type as one of the other components you will need to remove that component from the config before running the new component. For example, if the new component is a SLAM then the old SLAM needs to be removed from the config. See Building ILLIXR for more details on the config file. plugin_groups: - !include \"rt_slam_plugins.yaml\" - !include \"core_plugins.yaml\" - plugin_group: - path: /PATH/TO/NEW/PLUGIN - path: ground_truth_slam/ - path: gldemo/ - path: debugview/ data: subpath: mav0 relative_to: archive_path: download_url: 'http://robotics.ethz.ch/~asl-datasets/ijrr_euroc_mav_dataset/vicon_room1/V1_02_medium/V1_02_medium.zip' demo_data: demo_data/ loader: name: native # command: gdb -q --args %a profile: opt Finally, run ILLIXR with your new plugin with the following command: ./runner.sh configs/native.yaml This is all that is required to be a plugin which can be loaded by Spindle in the ILLIXR runtime. Reading and writing from Phonebook and Switchboard is optional, but nearly every plugin does it. See default_plugins.md for more details. First, we can query the phonebook to get various services including switchboard . Then we query switchboard for event-streams (topics). We will read topic1 , write to topic2 , and schedule computation on topic 3 . See the API documentation for phonebook and switchboard for more details. #include \"common/phonebook.hpp\" #include \"common/plugin.hpp\" #include \"common/threadloop.hpp\" /* When datatypes have to be common across plugins * (e.g. a phonebook service or switchboard topic), * they are defined in this header, * which is accessible to all plugins. */ #include \"common/data_format.hpp\" class plugin_name : public threadloop { public: /* After the constructor, C++ permits a list of member-constructors. * We use uniform initialization (curly-braces) [1] instead of parens to * avoid ambiguity [2]. * We put the comma at the start of the line, so that lines can be copied around * or deleted freely (except for the first). * * [1]: https://en.wikipedia.org/wiki/C%2B%2B11#Uniform_initialization * [2]: https://en.wikipedia.org/wiki/Most_vexing_parse */ plugin_name(std::string name_, phonebook* pb_) : threadloop{name_, pb_} /// Find the switchboard in phonebook , sb{pb->lookup_impl<switchboard>()} /// Create a handle to a topic in switchboard for subscribing , topic1{sb->get_reader<topic1_type>(\"topic1\")} /// Create a handle to a topic in switchboard for publishing , topic2{sb->get_writer<topic2_type>(\"topic2\")} { /// Read topic 1 switchboard::ptr<const topic1_type> event1 = topic1.get_ro(); /// Write to topic 2 topic2.put( topic2.allocate<topic2_type>( arg_1, // topic2_type::topic2_type() arg_type_1 ..., // ... arg_k // topic2_type::topic2_type() arg_type_k ) ); /// Read topic 3 synchronously sb->schedule<topic3_type>( get_name(), \"topic3\", [&](switchboard::ptr<const topic3_type> event3, std::size_t) { /* This is a [lambda expression][1] * * [1]: https://en.cppreference.com/w/cpp/language/lambda */ std::cout << \"Got a new event on topic3: \" << event3 << std::endl; callback(event3); } ); } virtual void _p_one_iteration override() { std::cout << \"Running\" << std::endl; auto target = std::chrono::system_clock::now() + std::chrono::milliseconds{10}; reliable_sleep(target); } private: const std::shared_ptr<switchboard> sb; switchboard::reader<topic1_type> topic1; switchboard::writer<topic2> topic2; }; /// This line makes the plugin importable by Spindle PLUGIN_MAIN(plugin_name);","title":"Tutorial"},{"location":"legacy/v1/","text":"This is the final release of ILLIXR that contains only standalone components. All future releases will contain both the components and the runtime, and can be found here . ILLIXR ILLIXR (pronounced like elixir) is an open-source Extended Reality (XR) benchmark suite. It contains several core state-of-the-art components of a generic XR pipeline, components that are required in most, if not all, XR applications. We use the term components and not kernels or computations because each component of ILLIXR is an entire application in itself, and consists of many kernels and computations. At the moment, ILLIXR contains the following state-of-the-art components, all of which are included as sub-modules in this repo. Simultaneous Localization and Mapping Scene reconstruction Eye tracking Ambisonic encoding Ambisonic manipulation and binauralization Lens distortion correction Chromatic aberration correction Time warp Computational holography for adaptive multi-focal displays We plan on adding more components to ILLIXR (e.g., graphics and multiple versions for individual components), including a runtime to integrate all of the components into a full XR system. Our goal is not to create a commercial quality XR product for current hardware. Instead, the goal for ILLIXR is to advance computer architecture, systems, and hardware-software co-design research for XR by making available key state-of-the-art components of both modern and future XR applications. Many of the current components of ILLIXR were developed by domain experts and obtained from publicly available repositories. They were modified for one or more of the following reasons: fixing compilation, adding features, or removing extraneous code or dependencies. Each component not developed by us is available as a forked github repository for proper attribution to its authors. Detailed descriptions of each component, including performance and energy profiles, can be found in our paper . Publications We request that you cite our following paper when you use ILLIXR for a publication. We would also appreciate it if you send us a citation once your work has been published. @misc{HuzaifaDesai2020, title={Exploring Extended Reality with ILLIXR: A new Playground for Architecture Research}, author={Muhammad Huzaifa and Rishi Desai and Xutao Jiang and Joseph Ravichandran and Finn Sinclair and Sarita V. Adve}, year={2020}, eprint={2004.04643}, primaryClass={cs.DC} } Setup Each component of ILLIXR is packaged as its own repository for modularity. Please refer to the setup instructions of each individual component in benchmark/ . To clone this repo use the following command: git clone https://github.com/ILLIXR/ILLIXR.git --recursive Acknowledgements Muhammad Huzaifa led the development of ILLIXR in Sarita Adve\u2019s research group at the University of Illinois at Urbana-Champaign. Other major contributors include Rishi Desai, Samuel Grayson, Xutao Jiang, Ying Jing, Fang Lu, Joseph Ravichandran, and Finn Sinclair. ILLIXR came together after many consultations with researchers and practitioners in many domains: audio, graphics, optics, robotics, signal processing, and extended reality systems. We are deeply grateful for all of these discussions and specifically to the following: Wei Cu, Aleksandra Faust, Liang Gao, Matt Horsnell, Amit Jindal, Steve LaValle, Steve Lovegrove, Andrew Maimone, Vegard \u00d8ye, Martin Persson, Archontis Politis, Eric Shaffer, Paris Smaragdis, Sachin Talathi, and Chris Widdowson. The development of ILLIXR was supported by the Applications Driving Architectures (ADA) Research Center, a JUMP Center co-sponsored by SRC and DARPA, the Center for Future Architectures Research (C-FAR), one of the six centers of STARnet, a Semiconductor Research Corporation program sponsored by MARCO and DARPA, and by a Google Faculty Research Award. The development of ILLIXR was also aided by generous hardware and software donations from ARM and NVIDIA. Facebook Reality Labs provided the OpenEDS Semantic Segmentation Dataset . Wesley Darvin came up with the name for ILLIXR. Abdulrahman Mahmoud helped with the design of this website. Licensing Structure ILLIXR is available as open-source software under the University of Illinois/NCSA Open Source License . As mentioned above, ILLIXR largely consists of components developed by domain experts and modified for the purposes of inclusion in ILLIXR. However, ILLIXR does contain software developed solely by us. The NCSA license is limited to only this software . The external libraries and softwares included in ILLIXR each have their own licenses and must be used according to those licenses: Open-VINS - GNU General Public License v3.0 ElasticFusion - ElasticFusion license RITnet - MIT License libspatialaudio - GNU Lesser General Public License v2.1 HOTlab - GNU Lesser General Public License v3.0 Get In Touch Whether you are a computer architect, a systems person, an XR application developer, or just anyone interested in XR, we would love to hear your feedback on ILLIXR! ILLIXR is a living benchmark suite and we would like to both refine existing components and add new ones. We believe ILLIXR has the opportunity to drive future computer architecture and systems research for XR, and can benefit from contributions from other researchers and organizations. If you would like to be a part of this effort, please contact us at illixr at cs dot illinois dot edu .","title":"ILLIXR (Version 1)"},{"location":"legacy/v1/#illixr","text":"ILLIXR (pronounced like elixir) is an open-source Extended Reality (XR) benchmark suite. It contains several core state-of-the-art components of a generic XR pipeline, components that are required in most, if not all, XR applications. We use the term components and not kernels or computations because each component of ILLIXR is an entire application in itself, and consists of many kernels and computations. At the moment, ILLIXR contains the following state-of-the-art components, all of which are included as sub-modules in this repo. Simultaneous Localization and Mapping Scene reconstruction Eye tracking Ambisonic encoding Ambisonic manipulation and binauralization Lens distortion correction Chromatic aberration correction Time warp Computational holography for adaptive multi-focal displays We plan on adding more components to ILLIXR (e.g., graphics and multiple versions for individual components), including a runtime to integrate all of the components into a full XR system. Our goal is not to create a commercial quality XR product for current hardware. Instead, the goal for ILLIXR is to advance computer architecture, systems, and hardware-software co-design research for XR by making available key state-of-the-art components of both modern and future XR applications. Many of the current components of ILLIXR were developed by domain experts and obtained from publicly available repositories. They were modified for one or more of the following reasons: fixing compilation, adding features, or removing extraneous code or dependencies. Each component not developed by us is available as a forked github repository for proper attribution to its authors. Detailed descriptions of each component, including performance and energy profiles, can be found in our paper .","title":"ILLIXR"},{"location":"legacy/v1/#publications","text":"We request that you cite our following paper when you use ILLIXR for a publication. We would also appreciate it if you send us a citation once your work has been published. @misc{HuzaifaDesai2020, title={Exploring Extended Reality with ILLIXR: A new Playground for Architecture Research}, author={Muhammad Huzaifa and Rishi Desai and Xutao Jiang and Joseph Ravichandran and Finn Sinclair and Sarita V. Adve}, year={2020}, eprint={2004.04643}, primaryClass={cs.DC} }","title":"Publications"},{"location":"legacy/v1/#setup","text":"Each component of ILLIXR is packaged as its own repository for modularity. Please refer to the setup instructions of each individual component in benchmark/ . To clone this repo use the following command: git clone https://github.com/ILLIXR/ILLIXR.git --recursive","title":"Setup"},{"location":"legacy/v1/#acknowledgements","text":"Muhammad Huzaifa led the development of ILLIXR in Sarita Adve\u2019s research group at the University of Illinois at Urbana-Champaign. Other major contributors include Rishi Desai, Samuel Grayson, Xutao Jiang, Ying Jing, Fang Lu, Joseph Ravichandran, and Finn Sinclair. ILLIXR came together after many consultations with researchers and practitioners in many domains: audio, graphics, optics, robotics, signal processing, and extended reality systems. We are deeply grateful for all of these discussions and specifically to the following: Wei Cu, Aleksandra Faust, Liang Gao, Matt Horsnell, Amit Jindal, Steve LaValle, Steve Lovegrove, Andrew Maimone, Vegard \u00d8ye, Martin Persson, Archontis Politis, Eric Shaffer, Paris Smaragdis, Sachin Talathi, and Chris Widdowson. The development of ILLIXR was supported by the Applications Driving Architectures (ADA) Research Center, a JUMP Center co-sponsored by SRC and DARPA, the Center for Future Architectures Research (C-FAR), one of the six centers of STARnet, a Semiconductor Research Corporation program sponsored by MARCO and DARPA, and by a Google Faculty Research Award. The development of ILLIXR was also aided by generous hardware and software donations from ARM and NVIDIA. Facebook Reality Labs provided the OpenEDS Semantic Segmentation Dataset . Wesley Darvin came up with the name for ILLIXR. Abdulrahman Mahmoud helped with the design of this website.","title":"Acknowledgements"},{"location":"legacy/v1/#licensing-structure","text":"ILLIXR is available as open-source software under the University of Illinois/NCSA Open Source License . As mentioned above, ILLIXR largely consists of components developed by domain experts and modified for the purposes of inclusion in ILLIXR. However, ILLIXR does contain software developed solely by us. The NCSA license is limited to only this software . The external libraries and softwares included in ILLIXR each have their own licenses and must be used according to those licenses: Open-VINS - GNU General Public License v3.0 ElasticFusion - ElasticFusion license RITnet - MIT License libspatialaudio - GNU Lesser General Public License v2.1 HOTlab - GNU Lesser General Public License v3.0","title":"Licensing Structure"},{"location":"legacy/v1/#get-in-touch","text":"Whether you are a computer architect, a systems person, an XR application developer, or just anyone interested in XR, we would love to hear your feedback on ILLIXR! ILLIXR is a living benchmark suite and we would like to both refine existing components and add new ones. We believe ILLIXR has the opportunity to drive future computer architecture and systems research for XR, and can benefit from contributions from other researchers and organizations. If you would like to be a part of this effort, please contact us at illixr at cs dot illinois dot edu .","title":"Get In Touch"},{"location":"legacy/v1/README-audio/","text":"Audio Pipeline Part of ILLIXR , the Illinios Extended Reality Benchmark Suite. The audio pipeline is responsible for both recording and playing back spatialized audio for XR. Build This version simplifies the build process to automate building of both libspatialaudio and audio pipeline itself. If you have a old version of this module and updating to the new version doesn't build correctly, you may need to purge the old module and clone this new version again. Build debug: make or make solo.dbg Build release: make solo.opt If you are switching between builds, please do make deepclean Also note that release build (-O3) would show great performance improvement over debug build. Usage ./solo.dbg <number of 1024-sample-block to process> <optional: decode/encode> Number of blocks to process is a required parameter. Decode/encode specifies the different audio processing procedures to take on, which is specificially designed for profiling. No output would be generated. If encode or decode is not specified, the code will do both encode and decode on preset input sound sample files and generate a spatialized output audio at output.wav . Example: ./solo.dbg 500 This will generate a ~10 seconds spatialized audio output from two sound samples under ./sample/ ./solo.dbg 2000 encode This will encode 2000 sample blocks of audio input into ambisonics format. Notes The input and output are hardcoded to be 48000 sample rate, 16-bit PCM wav file. Also if you want to hear the output sound, limit the process sample blocks so that the output is not longer than input! Otherwise, garbage sound samples would be generated. Components libspatialaudio Submodule libspatialaudio provides the backend library for Ambisonics encoding, decoding, rotation, zoom, and binauralizer (HRTF included). audio pipeline code sound.cpp Describes a sound source in the ambisonics sound-field, including the input file for the sound source and its position in the sound-field. audio.cpp Encapsulate preset processing steps of sound source reading, encoding, rotating, zooming, and decoding. License This code is available under the University of Illinois/NCSA Open Source License. The sound samples provided in ./sample/ are available under the Creative Commons 0 license.","title":"Audio Pipeline (Version 1)"},{"location":"legacy/v1/README-audio/#audio-pipeline","text":"Part of ILLIXR , the Illinios Extended Reality Benchmark Suite. The audio pipeline is responsible for both recording and playing back spatialized audio for XR.","title":"Audio Pipeline"},{"location":"legacy/v1/README-audio/#build","text":"This version simplifies the build process to automate building of both libspatialaudio and audio pipeline itself. If you have a old version of this module and updating to the new version doesn't build correctly, you may need to purge the old module and clone this new version again. Build debug: make or make solo.dbg Build release: make solo.opt If you are switching between builds, please do make deepclean Also note that release build (-O3) would show great performance improvement over debug build.","title":"Build"},{"location":"legacy/v1/README-audio/#usage","text":"./solo.dbg <number of 1024-sample-block to process> <optional: decode/encode> Number of blocks to process is a required parameter. Decode/encode specifies the different audio processing procedures to take on, which is specificially designed for profiling. No output would be generated. If encode or decode is not specified, the code will do both encode and decode on preset input sound sample files and generate a spatialized output audio at output.wav .","title":"Usage"},{"location":"legacy/v1/README-audio/#example","text":"./solo.dbg 500 This will generate a ~10 seconds spatialized audio output from two sound samples under ./sample/ ./solo.dbg 2000 encode This will encode 2000 sample blocks of audio input into ambisonics format.","title":"Example:"},{"location":"legacy/v1/README-audio/#notes","text":"The input and output are hardcoded to be 48000 sample rate, 16-bit PCM wav file. Also if you want to hear the output sound, limit the process sample blocks so that the output is not longer than input! Otherwise, garbage sound samples would be generated.","title":"Notes"},{"location":"legacy/v1/README-audio/#components","text":"","title":"Components"},{"location":"legacy/v1/README-audio/#libspatialaudio","text":"Submodule libspatialaudio provides the backend library for Ambisonics encoding, decoding, rotation, zoom, and binauralizer (HRTF included).","title":"libspatialaudio"},{"location":"legacy/v1/README-audio/#audio-pipeline-code","text":"","title":"audio pipeline code"},{"location":"legacy/v1/README-audio/#soundcpp","text":"Describes a sound source in the ambisonics sound-field, including the input file for the sound source and its position in the sound-field.","title":"sound.cpp"},{"location":"legacy/v1/README-audio/#audiocpp","text":"Encapsulate preset processing steps of sound source reading, encoding, rotating, zooming, and decoding.","title":"audio.cpp"},{"location":"legacy/v1/README-audio/#license","text":"This code is available under the University of Illinois/NCSA Open Source License. The sound samples provided in ./sample/ are available under the Creative Commons 0 license.","title":"License"},{"location":"legacy/v1/README-efusion/","text":"ElasticFusion Real-time dense visual SLAM system capable of capturing comprehensive dense globally consistent surfel-based maps of room scale environments explored using an RGB-D camera. Part of ILLIXR , the Illinios Extended Reality Benchmark Suite. This version of ElasticFusion has been modified to enable fast odometry and disable the GUI. Please use the following command to replicate the results from the paper: ./ElasticFusion -l dyson_lab.klg -fo -nso -sc -q The description of each flag is provided in \"How Do I Use It?\" below. Related Publications Please cite this work if you make use of our system in any of your own endeavors: ElasticFusion: Real-Time Dense SLAM and Light Source Estimation , T. Whelan, R. F. Salas-Moreno, B. Glocker, A. J. Davison and S. Leutenegger , IJRR '16 ElasticFusion: Dense SLAM Without A Pose Graph , T. Whelan, S. Leutenegger, R. F. Salas-Moreno, B. Glocker and A. J. Davison , RSS '15 1. What do I need to build it? 1.1. Ubuntu Ubuntu 14.04, 15.04 or 16.04 (Though many other linux distros will work fine) CMake OpenGL CUDA >= 7.0 OpenNI2 SuiteSparse Eigen zlib libjpeg Pangolin librealsense - Optional (for Intel RealSense cameras) Firstly, add nVidia's official CUDA repository to your apt sources, then run the following command to pull in most dependencies from the official repos: sudo apt-get install -y cmake-qt-gui git build-essential libusb-1.0-0-dev libudev-dev openjdk-7-jdk freeglut3-dev libglew-dev cuda-7-5 libsuitesparse-dev libeigen3-dev zlib1g-dev libjpeg-dev Afterwards install OpenNI2 and Pangolin from source. Note, you may need to manually tell CMake where OpenNI2 is since Occipital's fork does not have an install option. It is important to build Pangolin last so that it can find some of the libraries it has optional dependencies on. When you have all of the dependencies installed, build the Core followed by the GUI. 1.2. Windows - Visual Studio Windows 7/10 with Visual Studio 2013 Update 5 (Though other configurations may work) CMake OpenGL CUDA >= 7.0 OpenNI2 SuiteSparse Eigen Pangolin zlib (Pangolin can automatically download and build this) libjpeg (Pangolin can automatically download and build this) librealsense - Optional (for Intel RealSense cameras) Firstly install cmake and cuda. Then download and build from source OpenNI2, SuiteSparse. Next download Eigen (no need to build it since it is a header-only library). Then download and build from source Pangolin but pay attention to the following cmake settings. There will be a lot of dependencies where path was not found. That is OK except OPENNI2 and EIGEN3 (those should be set to valid paths). You also need to set MSVC_USE_STATIC_CRT to false in order to correctly link to ElasticFusion projects. Also, you can set BUILD_EXAMPLES to false since we don't need them and some were crashing on my machine. Finally, build Core and GUI. 2. Is there an easier way to build it? Yes, if you run the build.sh script on a fresh clean install of Ubuntu 14.04, 15.04, or 16.04, enter your password for sudo a few times and wait a few minutes all dependencies will get downloaded and installed and it should build everything correctly. This has not been tested on anything but fresh installs, so I would advise using it with caution if you already have some of the dependencies installed. 3. Installation issues #include <Eigen/Core> not found sudo ln -sf /usr/include/eigen3/Eigen /usr/include/Eigen sudo ln -sf /usr/include/eigen3/unsupported /usr/include/unsupported invalid use of incomplete type \u2018const struct Eigen ... Pangolin must be installed AFTER all the other libraries to make use of optional dependencies. GLSL 3.30 is not supported. Supported versions are 1.10, 1.20, 1.30, 1.00 ES and 3.00 ES Make sure you are running ElasticFusion on your nVidia GPU. In particular, if you have an Optimus GPU - If you use Prime, follow instructions here - If you use Bumblebee, remember to run as optirun ./ElasticFusion 4. How do I use it? There are three subprojects in the repo: The Core is the main engine which builds into a shared library that you can link into other projects and treat like an API. The GUI is the graphical interface used to run the system on either live sensor data or a logged data file. The GPUTest is a small benchmarking program you can use to tune the CUDA kernel launch parameters used in the main engine. The GUI ( ElasticFusion ) can take a bunch of parameters when launching it from the command line. They are as follows: -cal : Loads a camera calibration file specified as fx fy cx cy . -l : Processes the specified .klg log file. -p : Loads ground truth poses to use instead of estimated pose. -c : Surfel confidence threshold (default 10 ). -d : Cutoff distance for depth processing (default 3 m). -i : Relative ICP/RGB tracking weight (default 10 ). -ie : Local loop closure residual threshold (default 5e-05 ). -ic : Local loop closure inlier threshold (default 35000 ). -cv : Local loop closure covariance threshold (default 1e-05 ). -pt : Global loop closure photometric threshold (default 115 ). -ft : Fern encoding threshold (default 0.3095 ). -t : Time window length (default 200 ). -s : Frames to skip at start of log. -e : Cut off frame of log. -f : Flip RGB/BGR. -icl : Enable this if using the ICL-NUIM dataset (flips normals to account for negative focal length on that data). -o : Open loop mode. -rl : Enable relocalisation. -fs : Frame skip if processing a log to simulate real-time. -q : Quit when finished a log. -fo : Fast odometry (single level pyramid). -nso : Disables SO(3) pre-alignment in tracking. -r : Rewind and loop log forever. -ftf : Do frame-to-frame RGB tracking. -sc : Showcase mode (minimal GUI). Essentially by default ./ElasticFusion will try run off an attached ASUS sensor live. You can provide a .klg log file instead with the -l parameter. You can capture .klg format logs using either Logger1 or Logger2 . 5. How do I just use the Core API? The libefusion.so shared library which gets built by the Core is what you want to link against. An example of this can be seen in the GUI code. Essentially all you need to do is utilise the provided Findefusion.cmake file in GUI/src and include the following in your CMakeLists.txt file: find_package(efusion REQUIRED) include_directories(${EFUSION_INCLUDE_DIR}) target_link_libraries(MyProject ${EFUSION_LIBRARY}) To then use the Core API, make sure to include the header file in your source file: #include <ElasticFusion.h> Initialise the static configuration parameters once somewhere at the start of your program (this smells , but whatever): Resolution::getInstance(640, 480); Intrinsics::getInstance(528, 528, 320, 240); Create an OpenGL context before creating an ElasticFusion object, as ElasticFusion uses OpenGL internally. You can do this whatever way you wish, using Pangolin is probably easiest given it's a dependency: pangolin::Params windowParams; windowParams.Set(\"SAMPLE_BUFFERS\", 0); windowParams.Set(\"SAMPLES\", 0); pangolin::CreateWindowAndBind(\"Main\", 1280, 800, windowParams); Make an ElasticFusion object and start using it: ElasticFusion eFusion; eFusion.processFrame(rgb, depth, timestamp, currentPose, weightMultiplier); See the source code of MainController.cpp in the GUI source to see more usage. 6. Datasets We have provided a sample dataset which you can run easily with ElasticFusion for download here . Launch it as follows: ./ElasticFusion -l dyson_lab.klg 7. License ElasticFusion is freely available for non-commercial use only. Full terms and conditions which govern its use are detailed here and in the LICENSE.txt file. 8. FAQ What are the hardware requirements? A very fast nVidia GPU (3.5TFLOPS+) , and a fast CPU (something like an i7). If you want to use a non-nVidia GPU you can rewrite the tracking code or substitute it with something else, as the rest of the pipeline is actually written in the OpenGL Shading Language. How can I get performance statistics? Download Stopwatch and run StopwatchViewer at the same time as ElasticFusion. I ran a large dataset and got assert(graph.size() / 16 < MAX_NODES) failed Currently there's a limit on the number of nodes in the deformation graph down to lazy coding (using a really wide texture instead of a proper 2D one). So we're bound by the maximum dimension of a texture, which is 16384 on modern cards/OpenGL. Either fix the code so this isn't a problem any more, or increase the modulo factor in Shaders/sample.geom . I have a nice new laptop with a good GPU but it's still slow If your laptop is running on battery power the GPU will throttle down to save power, so that's unlikely to work (as an aside, Kintinuous will run at 30Hz on a modern laptop on battery power these days). You can try disabling SO(3) pre-alignment, enabling fast odometry, only using either ICP or RGB tracking and not both, running in open loop mode or disabling the tracking pyramid. All of these will cost you accuracy. I saved a map, how can I view it? Download Meshlab . Select Render->Shaders->Splatting. The map keeps getting corrupted - tracking is failing - loop closures are incorrect/not working Firstly, if you're running live and not processing a log file, ensure you're hitting 30Hz, this is important. Secondly, you cannot move the sensor extremely fast because this violates the assumption behind projective data association. In addition to this, you're probably using a primesense, which means you're suffering from motion blur, unsynchronised cameras and rolling shutter. All of these are aggravated by fast motion and hinder tracking performance. If you're not getting loop closures and expecting some, pay attention to the inlier and residual graphs in the bottom right, these are an indicator of how closae you are to a local loop closure. For global loop closures, you're depending on fern keyframe encoding to save you, which like all appearance-based place recognition methods, has its limitations. Is there a ROS bridge/node? No. The system relies on an extremely fast and tight coupling between the mapping and tracking on the GPU, which I don't believe ROS supports natively in terms of message passing. This doesn't seem to work like it did in the videos/papers A substantial amount of refactoring was carried out in order to open source this system, including rewriting a lot of functionality to avoid certain licenses and reduce dependencies. Although great care was taken during this process, it is possible that performance regressions were introduced and have not yet been discovered.","title":"ElasticFusion (Version 1)"},{"location":"legacy/v1/README-efusion/#elasticfusion","text":"Real-time dense visual SLAM system capable of capturing comprehensive dense globally consistent surfel-based maps of room scale environments explored using an RGB-D camera. Part of ILLIXR , the Illinios Extended Reality Benchmark Suite. This version of ElasticFusion has been modified to enable fast odometry and disable the GUI. Please use the following command to replicate the results from the paper: ./ElasticFusion -l dyson_lab.klg -fo -nso -sc -q The description of each flag is provided in \"How Do I Use It?\" below.","title":"ElasticFusion"},{"location":"legacy/v1/README-efusion/#related-publications","text":"Please cite this work if you make use of our system in any of your own endeavors: ElasticFusion: Real-Time Dense SLAM and Light Source Estimation , T. Whelan, R. F. Salas-Moreno, B. Glocker, A. J. Davison and S. Leutenegger , IJRR '16 ElasticFusion: Dense SLAM Without A Pose Graph , T. Whelan, S. Leutenegger, R. F. Salas-Moreno, B. Glocker and A. J. Davison , RSS '15","title":"Related Publications"},{"location":"legacy/v1/README-efusion/#1-what-do-i-need-to-build-it","text":"","title":"1. What do I need to build it?"},{"location":"legacy/v1/README-efusion/#11-ubuntu","text":"Ubuntu 14.04, 15.04 or 16.04 (Though many other linux distros will work fine) CMake OpenGL CUDA >= 7.0 OpenNI2 SuiteSparse Eigen zlib libjpeg Pangolin librealsense - Optional (for Intel RealSense cameras) Firstly, add nVidia's official CUDA repository to your apt sources, then run the following command to pull in most dependencies from the official repos: sudo apt-get install -y cmake-qt-gui git build-essential libusb-1.0-0-dev libudev-dev openjdk-7-jdk freeglut3-dev libglew-dev cuda-7-5 libsuitesparse-dev libeigen3-dev zlib1g-dev libjpeg-dev Afterwards install OpenNI2 and Pangolin from source. Note, you may need to manually tell CMake where OpenNI2 is since Occipital's fork does not have an install option. It is important to build Pangolin last so that it can find some of the libraries it has optional dependencies on. When you have all of the dependencies installed, build the Core followed by the GUI.","title":"1.1. Ubuntu"},{"location":"legacy/v1/README-efusion/#12-windows-visual-studio","text":"Windows 7/10 with Visual Studio 2013 Update 5 (Though other configurations may work) CMake OpenGL CUDA >= 7.0 OpenNI2 SuiteSparse Eigen Pangolin zlib (Pangolin can automatically download and build this) libjpeg (Pangolin can automatically download and build this) librealsense - Optional (for Intel RealSense cameras) Firstly install cmake and cuda. Then download and build from source OpenNI2, SuiteSparse. Next download Eigen (no need to build it since it is a header-only library). Then download and build from source Pangolin but pay attention to the following cmake settings. There will be a lot of dependencies where path was not found. That is OK except OPENNI2 and EIGEN3 (those should be set to valid paths). You also need to set MSVC_USE_STATIC_CRT to false in order to correctly link to ElasticFusion projects. Also, you can set BUILD_EXAMPLES to false since we don't need them and some were crashing on my machine. Finally, build Core and GUI.","title":"1.2. Windows - Visual Studio"},{"location":"legacy/v1/README-efusion/#2-is-there-an-easier-way-to-build-it","text":"Yes, if you run the build.sh script on a fresh clean install of Ubuntu 14.04, 15.04, or 16.04, enter your password for sudo a few times and wait a few minutes all dependencies will get downloaded and installed and it should build everything correctly. This has not been tested on anything but fresh installs, so I would advise using it with caution if you already have some of the dependencies installed.","title":"2. Is there an easier way to build it?"},{"location":"legacy/v1/README-efusion/#3-installation-issues","text":"#include <Eigen/Core> not found sudo ln -sf /usr/include/eigen3/Eigen /usr/include/Eigen sudo ln -sf /usr/include/eigen3/unsupported /usr/include/unsupported invalid use of incomplete type \u2018const struct Eigen ... Pangolin must be installed AFTER all the other libraries to make use of optional dependencies. GLSL 3.30 is not supported. Supported versions are 1.10, 1.20, 1.30, 1.00 ES and 3.00 ES Make sure you are running ElasticFusion on your nVidia GPU. In particular, if you have an Optimus GPU - If you use Prime, follow instructions here - If you use Bumblebee, remember to run as optirun ./ElasticFusion","title":"3. Installation issues"},{"location":"legacy/v1/README-efusion/#4-how-do-i-use-it","text":"There are three subprojects in the repo: The Core is the main engine which builds into a shared library that you can link into other projects and treat like an API. The GUI is the graphical interface used to run the system on either live sensor data or a logged data file. The GPUTest is a small benchmarking program you can use to tune the CUDA kernel launch parameters used in the main engine. The GUI ( ElasticFusion ) can take a bunch of parameters when launching it from the command line. They are as follows: -cal : Loads a camera calibration file specified as fx fy cx cy . -l : Processes the specified .klg log file. -p : Loads ground truth poses to use instead of estimated pose. -c : Surfel confidence threshold (default 10 ). -d : Cutoff distance for depth processing (default 3 m). -i : Relative ICP/RGB tracking weight (default 10 ). -ie : Local loop closure residual threshold (default 5e-05 ). -ic : Local loop closure inlier threshold (default 35000 ). -cv : Local loop closure covariance threshold (default 1e-05 ). -pt : Global loop closure photometric threshold (default 115 ). -ft : Fern encoding threshold (default 0.3095 ). -t : Time window length (default 200 ). -s : Frames to skip at start of log. -e : Cut off frame of log. -f : Flip RGB/BGR. -icl : Enable this if using the ICL-NUIM dataset (flips normals to account for negative focal length on that data). -o : Open loop mode. -rl : Enable relocalisation. -fs : Frame skip if processing a log to simulate real-time. -q : Quit when finished a log. -fo : Fast odometry (single level pyramid). -nso : Disables SO(3) pre-alignment in tracking. -r : Rewind and loop log forever. -ftf : Do frame-to-frame RGB tracking. -sc : Showcase mode (minimal GUI). Essentially by default ./ElasticFusion will try run off an attached ASUS sensor live. You can provide a .klg log file instead with the -l parameter. You can capture .klg format logs using either Logger1 or Logger2 .","title":"4. How do I use it?"},{"location":"legacy/v1/README-efusion/#5-how-do-i-just-use-the-core-api","text":"The libefusion.so shared library which gets built by the Core is what you want to link against. An example of this can be seen in the GUI code. Essentially all you need to do is utilise the provided Findefusion.cmake file in GUI/src and include the following in your CMakeLists.txt file: find_package(efusion REQUIRED) include_directories(${EFUSION_INCLUDE_DIR}) target_link_libraries(MyProject ${EFUSION_LIBRARY}) To then use the Core API, make sure to include the header file in your source file: #include <ElasticFusion.h> Initialise the static configuration parameters once somewhere at the start of your program (this smells , but whatever): Resolution::getInstance(640, 480); Intrinsics::getInstance(528, 528, 320, 240); Create an OpenGL context before creating an ElasticFusion object, as ElasticFusion uses OpenGL internally. You can do this whatever way you wish, using Pangolin is probably easiest given it's a dependency: pangolin::Params windowParams; windowParams.Set(\"SAMPLE_BUFFERS\", 0); windowParams.Set(\"SAMPLES\", 0); pangolin::CreateWindowAndBind(\"Main\", 1280, 800, windowParams); Make an ElasticFusion object and start using it: ElasticFusion eFusion; eFusion.processFrame(rgb, depth, timestamp, currentPose, weightMultiplier); See the source code of MainController.cpp in the GUI source to see more usage.","title":"5. How do I just use the Core API?"},{"location":"legacy/v1/README-efusion/#6-datasets","text":"We have provided a sample dataset which you can run easily with ElasticFusion for download here . Launch it as follows: ./ElasticFusion -l dyson_lab.klg","title":"6. Datasets"},{"location":"legacy/v1/README-efusion/#7-license","text":"ElasticFusion is freely available for non-commercial use only. Full terms and conditions which govern its use are detailed here and in the LICENSE.txt file.","title":"7. License"},{"location":"legacy/v1/README-efusion/#8-faq","text":"What are the hardware requirements? A very fast nVidia GPU (3.5TFLOPS+) , and a fast CPU (something like an i7). If you want to use a non-nVidia GPU you can rewrite the tracking code or substitute it with something else, as the rest of the pipeline is actually written in the OpenGL Shading Language. How can I get performance statistics? Download Stopwatch and run StopwatchViewer at the same time as ElasticFusion. I ran a large dataset and got assert(graph.size() / 16 < MAX_NODES) failed Currently there's a limit on the number of nodes in the deformation graph down to lazy coding (using a really wide texture instead of a proper 2D one). So we're bound by the maximum dimension of a texture, which is 16384 on modern cards/OpenGL. Either fix the code so this isn't a problem any more, or increase the modulo factor in Shaders/sample.geom . I have a nice new laptop with a good GPU but it's still slow If your laptop is running on battery power the GPU will throttle down to save power, so that's unlikely to work (as an aside, Kintinuous will run at 30Hz on a modern laptop on battery power these days). You can try disabling SO(3) pre-alignment, enabling fast odometry, only using either ICP or RGB tracking and not both, running in open loop mode or disabling the tracking pyramid. All of these will cost you accuracy. I saved a map, how can I view it? Download Meshlab . Select Render->Shaders->Splatting. The map keeps getting corrupted - tracking is failing - loop closures are incorrect/not working Firstly, if you're running live and not processing a log file, ensure you're hitting 30Hz, this is important. Secondly, you cannot move the sensor extremely fast because this violates the assumption behind projective data association. In addition to this, you're probably using a primesense, which means you're suffering from motion blur, unsynchronised cameras and rolling shutter. All of these are aggravated by fast motion and hinder tracking performance. If you're not getting loop closures and expecting some, pay attention to the inlier and residual graphs in the bottom right, these are an indicator of how closae you are to a local loop closure. For global loop closures, you're depending on fern keyframe encoding to save you, which like all appearance-based place recognition methods, has its limitations. Is there a ROS bridge/node? No. The system relies on an extremely fast and tight coupling between the mapping and tracking on the GPU, which I don't believe ROS supports natively in terms of message passing. This doesn't seem to work like it did in the videos/papers A substantial amount of refactoring was carried out in order to open source this system, including rewriting a lot of functionality to avoid certain licenses and reduce dependencies. Although great care was taken during this process, it is possible that performance regressions were introduced and have not yet been discovered.","title":"8. FAQ"},{"location":"legacy/v1/README-hotlab/","text":"Computational Holography Part of ILLIXR , the Illinios Extended Reality Benchmark Suite. This component is responsible for calculating image holograms (per-pixel phase masks) using the Weighted Gerchberg\u2013Saxton (GSW) algorithm. Files generateHologram. cu generateHologram Host side kernel launch code. propagateToSpotPositions CUDA kernel that propagates phases from the SLM plane to the depth plane using Fresnel summation. propagateToSpotSum CUDA kernel that sums up the per-thread block results from the propagateToSpotPositions() kernel. propagateToSLM CUDA kernel that calculates the error function at the depth planes and updates the SLM phases. goldenHologram. cu The original hologram implementation. This implementation did not support arbitrary SLM sizes and colored holograms. Installation & Usage Under C/source/ make all make jetson make all compiles for the SM75 architecture, while make jetson compiles for SM70. To run this code on a older NVIDIA GPU, please change the SM architecture accordingly. To run our modified hologram code: ./hologram To run the original hologram code: ./goldenHologram License This code is available under the LGPL license.","title":"HOTlab (Version 1)"},{"location":"legacy/v1/README-hotlab/#computational-holography","text":"Part of ILLIXR , the Illinios Extended Reality Benchmark Suite. This component is responsible for calculating image holograms (per-pixel phase masks) using the Weighted Gerchberg\u2013Saxton (GSW) algorithm.","title":"Computational Holography"},{"location":"legacy/v1/README-hotlab/#files","text":"","title":"Files"},{"location":"legacy/v1/README-hotlab/#generatehologramcu","text":"","title":"generateHologram.cu"},{"location":"legacy/v1/README-hotlab/#generatehologram","text":"Host side kernel launch code.","title":"generateHologram"},{"location":"legacy/v1/README-hotlab/#propagatetospotpositions","text":"CUDA kernel that propagates phases from the SLM plane to the depth plane using Fresnel summation.","title":"propagateToSpotPositions"},{"location":"legacy/v1/README-hotlab/#propagatetospotsum","text":"CUDA kernel that sums up the per-thread block results from the propagateToSpotPositions() kernel.","title":"propagateToSpotSum"},{"location":"legacy/v1/README-hotlab/#propagatetoslm","text":"CUDA kernel that calculates the error function at the depth planes and updates the SLM phases.","title":"propagateToSLM"},{"location":"legacy/v1/README-hotlab/#goldenhologramcu","text":"The original hologram implementation. This implementation did not support arbitrary SLM sizes and colored holograms.","title":"goldenHologram.cu"},{"location":"legacy/v1/README-hotlab/#installation-usage","text":"Under C/source/ make all make jetson make all compiles for the SM75 architecture, while make jetson compiles for SM70. To run this code on a older NVIDIA GPU, please change the SM architecture accordingly. To run our modified hologram code: ./hologram To run the original hologram code: ./goldenHologram","title":"Installation &amp; Usage"},{"location":"legacy/v1/README-hotlab/#license","text":"This code is available under the LGPL license.","title":"License"},{"location":"legacy/v1/README-openvins/","text":"Part of ILLIXR , the Illinios Extended Reality Benchmark Suite. For instructions on how to use OpenVINS for ILLIXR, see ILLIXR_INSTRUCTIONS.MD Open VINS Welcome to the Open VINS project! The Open VINS project houses some core computer vision code along with a state-of-the art filter-based visual-inertial estimator. The core filter is an Extended Kalman filter which fuses inertial information with sparse visual feature tracks. These visual feature tracks are fused leveraging the Multi-State Constraint Kalman Filter (MSCKF) sliding window formulation which allows for 3D features to update the state estimate without directly estimating the feature states in the filter. Inspired by graph-based optimization systems, the included filter has modularity allowing for convenient covariance management with a proper type-based state system. Please take a look at the feature list below for full details on what the system supports. Github project page - https://github.com/rpng/open_vins Documentation - https://docs.openvins.com/ Getting started guide - https://docs.openvins.com/getting-started.html Publication reference - TBD News / Events August 21, 2019 - Open sourced ov_maplab for interfacing OpenVINS with the maplab library. August 15, 2019 - Initial release of OpenVINS repository and documentation website! Project Features Sliding window visual-inertial MSCKF Modular covariance type system Comprehensive documentation and derivations Extendable visual-inertial simulator On manifold SE(3) b-spline Arbitrary number of cameras Arbitrary sensor rate Automatic feature generation Five different feature representations Global XYZ Global inverse depth Anchored XYZ Anchored inverse depth Anchored MSCKF inverse depth Calibration of sensor intrinsics and extrinsics Camera to IMU transform Camera to IMU time offset Camera intrinsics Environmental SLAM feature OpenCV ARUCO tag SLAM features Sparse feature SLAM features Visual tracking support Monocular camera Stereo camera KLT or descriptor based Static IMU initialization (sfm will be open sourced later) Out of the box evaluation on EurocMav and TUM-VI datasets Extensive evaluation suite (ATE, RPE, NEES, RMSE, etc..) Credit / Licensing This code was written by the Robot Perception and Navigation Group (RPNG) at the University of Delaware. If you have any issues with the code please open an issue on our github page with relevant implementation details and references. For researchers that have leveraged or compared to this work, please cite the following: @article{TBD, author = {}, title = {}, journal = {}, volume = {}, year = {2019}, } The codebase is licensed under the GNU General Public License v3 (GPL-3) .","title":"OpenVINS (Version 1)"},{"location":"legacy/v1/README-openvins/#open-vins","text":"Welcome to the Open VINS project! The Open VINS project houses some core computer vision code along with a state-of-the art filter-based visual-inertial estimator. The core filter is an Extended Kalman filter which fuses inertial information with sparse visual feature tracks. These visual feature tracks are fused leveraging the Multi-State Constraint Kalman Filter (MSCKF) sliding window formulation which allows for 3D features to update the state estimate without directly estimating the feature states in the filter. Inspired by graph-based optimization systems, the included filter has modularity allowing for convenient covariance management with a proper type-based state system. Please take a look at the feature list below for full details on what the system supports. Github project page - https://github.com/rpng/open_vins Documentation - https://docs.openvins.com/ Getting started guide - https://docs.openvins.com/getting-started.html Publication reference - TBD","title":"Open VINS"},{"location":"legacy/v1/README-openvins/#news-events","text":"August 21, 2019 - Open sourced ov_maplab for interfacing OpenVINS with the maplab library. August 15, 2019 - Initial release of OpenVINS repository and documentation website!","title":"News / Events"},{"location":"legacy/v1/README-openvins/#project-features","text":"Sliding window visual-inertial MSCKF Modular covariance type system Comprehensive documentation and derivations Extendable visual-inertial simulator On manifold SE(3) b-spline Arbitrary number of cameras Arbitrary sensor rate Automatic feature generation Five different feature representations Global XYZ Global inverse depth Anchored XYZ Anchored inverse depth Anchored MSCKF inverse depth Calibration of sensor intrinsics and extrinsics Camera to IMU transform Camera to IMU time offset Camera intrinsics Environmental SLAM feature OpenCV ARUCO tag SLAM features Sparse feature SLAM features Visual tracking support Monocular camera Stereo camera KLT or descriptor based Static IMU initialization (sfm will be open sourced later) Out of the box evaluation on EurocMav and TUM-VI datasets Extensive evaluation suite (ATE, RPE, NEES, RMSE, etc..)","title":"Project Features"},{"location":"legacy/v1/README-openvins/#credit-licensing","text":"This code was written by the Robot Perception and Navigation Group (RPNG) at the University of Delaware. If you have any issues with the code please open an issue on our github page with relevant implementation details and references. For researchers that have leveraged or compared to this work, please cite the following: @article{TBD, author = {}, title = {}, journal = {}, volume = {}, year = {2019}, } The codebase is licensed under the GNU General Public License v3 (GPL-3) .","title":"Credit / Licensing"},{"location":"legacy/v1/README-ritnet/","text":"This is part of ILLIXR , the Illinios Extended Reality Benchmark Suite. The following explains how to use RITnet. The code is based on Python3, and the profiling results are based on test.py . For the testing images, the size per image should be 640x400 in gray scale. Please put them under Semantic_Segmentation_Dataset/test/images . RITnet RITnet is the winnning model of the OpenEDS Semantic Segmentation Challenge. If you use this code, please cite: @misc{chaudhary2019ritnet, title={RITnet: Real-time Semantic Segmentation of the Eye for Gaze Tracking}, author={Aayush K. Chaudhary and Rakshit Kothari and Manoj Acharya and Shusil Dangi and Nitinraj Nair and Reynold Bailey and Christopher Kanan and Gabriel Diaz and Jeff B. Pelz}, year={2019}, eprint={1910.00694}, archivePrefix={arXiv}, primaryClass={cs.CV} } Instructions: python train.py --help To train the model with densenet model: python train.py --model densenet --expname FINAL --bs 8 --useGPU True --dataset Semantic_Segmentation_Dataset/ To test the result: python test.py --model densenet --load best_model.pkl --bs 4 --dataset Semantic_Segmentation_Dataset/ If you type in python test.py , the batch size will be 8. Contents in the zip folder best_model.pkl :: Our final model (potential winner model) which contains all the weights in Float32 format (Number of Parameters 248900). requirements.txt :: Includes all the necessary packages for the source code to run environment.yml :: List of all packages and version of one of our system in which the code was run successfully. dataset.py ::Data loader and augmentation train.py ::Train code test.py ::Test code densenet.py ::Model code utils.py ::List of utility files opt.py ::List of arguments for argparser models.py ::List of all models starburst_black.png:: A fixed structured pattern (with translation) used on train images to handle cases such as multiple reflections.(Train Image: 000000240768.png) Starburst generation from train image 000000240768.pdf ::Procedure how starburst pattern is generated The requirements.txt file contains all the packages necessary for the code to run. We have also included an environment.yml file to recreate the conda environment we used. We have submitted two models from this version of code: Epoch: 151 Validation accuracy: 95.7780 Test accuracy: 95.276 (Potential Winner Model: Last Submission) Epoch: 117 Validation accuracy: 95.7023 Test accuracy: 95.159 (Our Second Last Submission) We could reach upto Epoch: 240 Validation accuracy: 95.7820 Test accuracy:NA (Not submitted: result after the deadline) The dataset.py contains data loader, preprocessing and post processing step Required Preprocessing for all images (test, train and validation set). Gamma correction by a factor of 0.8 local Contrast limited adaptive histogram equalization algorithm with clipLimit=1.5, tileGridSize=(8,8) Normalization [Mean 0.5, std=0.5] Train Image Augmentation Procedure Followed (Not Required during test) Random horizontal flip with 50% probability. Starburst pattern augmentation with 20% probability. Random length lines (1 to 9) augmentation around a random center with 20% probability. Gaussian blur with kernel size (7,7) and random sigma (2 to 7) with 20% probability. Translation of image and labels in any direction with random factor less than 20 with 20% probability. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 32, 640, 400] 320 Dropout-2 [-1, 32, 640, 400] 0 LeakyReLU-3 [-1, 32, 640, 400] 0 Conv2d-4 [-1, 32, 640, 400] 1,088 Conv2d-5 [-1, 32, 640, 400] 9,248 Dropout-6 [-1, 32, 640, 400] 0 LeakyReLU-7 [-1, 32, 640, 400] 0 Conv2d-8 [-1, 32, 640, 400] 2,112 Conv2d-9 [-1, 32, 640, 400] 9,248 Dropout-10 [-1, 32, 640, 400] 0 LeakyReLU-11 [-1, 32, 640, 400] 0 BatchNorm2d-12 [-1, 32, 640, 400] 64 DenseNet2D_down_block-13 [-1, 32, 640, 400] 0 AvgPool2d-14 [-1, 32, 320, 200] 0 Conv2d-15 [-1, 32, 320, 200] 9,248 Dropout-16 [-1, 32, 320, 200] 0 LeakyReLU-17 [-1, 32, 320, 200] 0 Conv2d-18 [-1, 32, 320, 200] 2,080 Conv2d-19 [-1, 32, 320, 200] 9,248 Dropout-20 [-1, 32, 320, 200] 0 LeakyReLU-21 [-1, 32, 320, 200] 0 Conv2d-22 [-1, 32, 320, 200] 3,104 Conv2d-23 [-1, 32, 320, 200] 9,248 Dropout-24 [-1, 32, 320, 200] 0 LeakyReLU-25 [-1, 32, 320, 200] 0 BatchNorm2d-26 [-1, 32, 320, 200] 64 DenseNet2D_down_block-27 [-1, 32, 320, 200] 0 AvgPool2d-28 [-1, 32, 160, 100] 0 Conv2d-29 [-1, 32, 160, 100] 9,248 Dropout-30 [-1, 32, 160, 100] 0 LeakyReLU-31 [-1, 32, 160, 100] 0 Conv2d-32 [-1, 32, 160, 100] 2,080 Conv2d-33 [-1, 32, 160, 100] 9,248 Dropout-34 [-1, 32, 160, 100] 0 LeakyReLU-35 [-1, 32, 160, 100] 0 Conv2d-36 [-1, 32, 160, 100] 3,104 Conv2d-37 [-1, 32, 160, 100] 9,248 Dropout-38 [-1, 32, 160, 100] 0 LeakyReLU-39 [-1, 32, 160, 100] 0 BatchNorm2d-40 [-1, 32, 160, 100] 64 DenseNet2D_down_block-41 [-1, 32, 160, 100] 0 AvgPool2d-42 [-1, 32, 80, 50] 0 Conv2d-43 [-1, 32, 80, 50] 9,248 Dropout-44 [-1, 32, 80, 50] 0 LeakyReLU-45 [-1, 32, 80, 50] 0 Conv2d-46 [-1, 32, 80, 50] 2,080 Conv2d-47 [-1, 32, 80, 50] 9,248 Dropout-48 [-1, 32, 80, 50] 0 LeakyReLU-49 [-1, 32, 80, 50] 0 Conv2d-50 [-1, 32, 80, 50] 3,104 Conv2d-51 [-1, 32, 80, 50] 9,248 Dropout-52 [-1, 32, 80, 50] 0 LeakyReLU-53 [-1, 32, 80, 50] 0 BatchNorm2d-54 [-1, 32, 80, 50] 64 DenseNet2D_down_block-55 [-1, 32, 80, 50] 0 AvgPool2d-56 [-1, 32, 40, 25] 0 Conv2d-57 [-1, 32, 40, 25] 9,248 Dropout-58 [-1, 32, 40, 25] 0 LeakyReLU-59 [-1, 32, 40, 25] 0 Conv2d-60 [-1, 32, 40, 25] 2,080 Conv2d-61 [-1, 32, 40, 25] 9,248 Dropout-62 [-1, 32, 40, 25] 0 LeakyReLU-63 [-1, 32, 40, 25] 0 Conv2d-64 [-1, 32, 40, 25] 3,104 Conv2d-65 [-1, 32, 40, 25] 9,248 Dropout-66 [-1, 32, 40, 25] 0 LeakyReLU-67 [-1, 32, 40, 25] 0 BatchNorm2d-68 [-1, 32, 40, 25] 64 DenseNet2D_down_block-69 [-1, 32, 40, 25] 0 Conv2d-70 [-1, 32, 80, 50] 2,080 Conv2d-71 [-1, 32, 80, 50] 9,248 Dropout-72 [-1, 32, 80, 50] 0 LeakyReLU-73 [-1, 32, 80, 50] 0 Conv2d-74 [-1, 32, 80, 50] 3,104 Conv2d-75 [-1, 32, 80, 50] 9,248 Dropout-76 [-1, 32, 80, 50] 0 LeakyReLU-77 [-1, 32, 80, 50] 0 DenseNet2D_up_block_concat-78 [-1, 32, 80, 50] 0 Conv2d-79 [-1, 32, 160, 100] 2,080 Conv2d-80 [-1, 32, 160, 100] 9,248 Dropout-81 [-1, 32, 160, 100] 0 LeakyReLU-82 [-1, 32, 160, 100] 0 Conv2d-83 [-1, 32, 160, 100] 3,104 Conv2d-84 [-1, 32, 160, 100] 9,248 Dropout-85 [-1, 32, 160, 100] 0 LeakyReLU-86 [-1, 32, 160, 100] 0 DenseNet2D_up_block_concat-87 [-1, 32, 160, 100] 0 Conv2d-88 [-1, 32, 320, 200] 2,080 Conv2d-89 [-1, 32, 320, 200] 9,248 Dropout-90 [-1, 32, 320, 200] 0 LeakyReLU-91 [-1, 32, 320, 200] 0 Conv2d-92 [-1, 32, 320, 200] 3,104 Conv2d-93 [-1, 32, 320, 200] 9,248 Dropout-94 [-1, 32, 320, 200] 0 LeakyReLU-95 [-1, 32, 320, 200] 0 DenseNet2D_up_block_concat-96 [-1, 32, 320, 200] 0 Conv2d-97 [-1, 32, 640, 400] 2,080 Conv2d-98 [-1, 32, 640, 400] 9,248 Dropout-99 [-1, 32, 640, 400] 0 LeakyReLU-100 [-1, 32, 640, 400] 0 Conv2d-101 [-1, 32, 640, 400] 3,104 Conv2d-102 [-1, 32, 640, 400] 9,248 Dropout-103 [-1, 32, 640, 400] 0 LeakyReLU-104 [-1, 32, 640, 400] 0 DenseNet2D_up_block_concat-105 [-1, 32, 640, 400] 0 Dropout-106 [-1, 32, 640, 400] 0 Conv2d-107 [-1, 4, 640, 400] 132 ================================================================ Total params: 248,900 Trainable params: 248,900 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.98 Forward/backward pass size (MB): 1920.41 Params size (MB): 0.95 Estimated Total Size (MB): 1922.34 ----------------------------------------------------------------","title":"RITnet (Version 1)"},{"location":"legacy/v1/README-ritnet/#ritnet","text":"RITnet is the winnning model of the OpenEDS Semantic Segmentation Challenge. If you use this code, please cite: @misc{chaudhary2019ritnet, title={RITnet: Real-time Semantic Segmentation of the Eye for Gaze Tracking}, author={Aayush K. Chaudhary and Rakshit Kothari and Manoj Acharya and Shusil Dangi and Nitinraj Nair and Reynold Bailey and Christopher Kanan and Gabriel Diaz and Jeff B. Pelz}, year={2019}, eprint={1910.00694}, archivePrefix={arXiv}, primaryClass={cs.CV} } Instructions: python train.py --help To train the model with densenet model: python train.py --model densenet --expname FINAL --bs 8 --useGPU True --dataset Semantic_Segmentation_Dataset/ To test the result: python test.py --model densenet --load best_model.pkl --bs 4 --dataset Semantic_Segmentation_Dataset/ If you type in python test.py , the batch size will be 8.","title":"RITnet"},{"location":"legacy/v1/README-ritnet/#contents-in-the-zip-folder","text":"best_model.pkl :: Our final model (potential winner model) which contains all the weights in Float32 format (Number of Parameters 248900). requirements.txt :: Includes all the necessary packages for the source code to run environment.yml :: List of all packages and version of one of our system in which the code was run successfully. dataset.py ::Data loader and augmentation train.py ::Train code test.py ::Test code densenet.py ::Model code utils.py ::List of utility files opt.py ::List of arguments for argparser models.py ::List of all models starburst_black.png:: A fixed structured pattern (with translation) used on train images to handle cases such as multiple reflections.(Train Image: 000000240768.png) Starburst generation from train image 000000240768.pdf ::Procedure how starburst pattern is generated The requirements.txt file contains all the packages necessary for the code to run. We have also included an environment.yml file to recreate the conda environment we used. We have submitted two models from this version of code: Epoch: 151 Validation accuracy: 95.7780 Test accuracy: 95.276 (Potential Winner Model: Last Submission) Epoch: 117 Validation accuracy: 95.7023 Test accuracy: 95.159 (Our Second Last Submission) We could reach upto Epoch: 240 Validation accuracy: 95.7820 Test accuracy:NA (Not submitted: result after the deadline) The dataset.py contains data loader, preprocessing and post processing step Required Preprocessing for all images (test, train and validation set). Gamma correction by a factor of 0.8 local Contrast limited adaptive histogram equalization algorithm with clipLimit=1.5, tileGridSize=(8,8) Normalization [Mean 0.5, std=0.5] Train Image Augmentation Procedure Followed (Not Required during test) Random horizontal flip with 50% probability. Starburst pattern augmentation with 20% probability. Random length lines (1 to 9) augmentation around a random center with 20% probability. Gaussian blur with kernel size (7,7) and random sigma (2 to 7) with 20% probability. Translation of image and labels in any direction with random factor less than 20 with 20% probability. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 32, 640, 400] 320 Dropout-2 [-1, 32, 640, 400] 0 LeakyReLU-3 [-1, 32, 640, 400] 0 Conv2d-4 [-1, 32, 640, 400] 1,088 Conv2d-5 [-1, 32, 640, 400] 9,248 Dropout-6 [-1, 32, 640, 400] 0 LeakyReLU-7 [-1, 32, 640, 400] 0 Conv2d-8 [-1, 32, 640, 400] 2,112 Conv2d-9 [-1, 32, 640, 400] 9,248 Dropout-10 [-1, 32, 640, 400] 0 LeakyReLU-11 [-1, 32, 640, 400] 0 BatchNorm2d-12 [-1, 32, 640, 400] 64 DenseNet2D_down_block-13 [-1, 32, 640, 400] 0 AvgPool2d-14 [-1, 32, 320, 200] 0 Conv2d-15 [-1, 32, 320, 200] 9,248 Dropout-16 [-1, 32, 320, 200] 0 LeakyReLU-17 [-1, 32, 320, 200] 0 Conv2d-18 [-1, 32, 320, 200] 2,080 Conv2d-19 [-1, 32, 320, 200] 9,248 Dropout-20 [-1, 32, 320, 200] 0 LeakyReLU-21 [-1, 32, 320, 200] 0 Conv2d-22 [-1, 32, 320, 200] 3,104 Conv2d-23 [-1, 32, 320, 200] 9,248 Dropout-24 [-1, 32, 320, 200] 0 LeakyReLU-25 [-1, 32, 320, 200] 0 BatchNorm2d-26 [-1, 32, 320, 200] 64 DenseNet2D_down_block-27 [-1, 32, 320, 200] 0 AvgPool2d-28 [-1, 32, 160, 100] 0 Conv2d-29 [-1, 32, 160, 100] 9,248 Dropout-30 [-1, 32, 160, 100] 0 LeakyReLU-31 [-1, 32, 160, 100] 0 Conv2d-32 [-1, 32, 160, 100] 2,080 Conv2d-33 [-1, 32, 160, 100] 9,248 Dropout-34 [-1, 32, 160, 100] 0 LeakyReLU-35 [-1, 32, 160, 100] 0 Conv2d-36 [-1, 32, 160, 100] 3,104 Conv2d-37 [-1, 32, 160, 100] 9,248 Dropout-38 [-1, 32, 160, 100] 0 LeakyReLU-39 [-1, 32, 160, 100] 0 BatchNorm2d-40 [-1, 32, 160, 100] 64 DenseNet2D_down_block-41 [-1, 32, 160, 100] 0 AvgPool2d-42 [-1, 32, 80, 50] 0 Conv2d-43 [-1, 32, 80, 50] 9,248 Dropout-44 [-1, 32, 80, 50] 0 LeakyReLU-45 [-1, 32, 80, 50] 0 Conv2d-46 [-1, 32, 80, 50] 2,080 Conv2d-47 [-1, 32, 80, 50] 9,248 Dropout-48 [-1, 32, 80, 50] 0 LeakyReLU-49 [-1, 32, 80, 50] 0 Conv2d-50 [-1, 32, 80, 50] 3,104 Conv2d-51 [-1, 32, 80, 50] 9,248 Dropout-52 [-1, 32, 80, 50] 0 LeakyReLU-53 [-1, 32, 80, 50] 0 BatchNorm2d-54 [-1, 32, 80, 50] 64 DenseNet2D_down_block-55 [-1, 32, 80, 50] 0 AvgPool2d-56 [-1, 32, 40, 25] 0 Conv2d-57 [-1, 32, 40, 25] 9,248 Dropout-58 [-1, 32, 40, 25] 0 LeakyReLU-59 [-1, 32, 40, 25] 0 Conv2d-60 [-1, 32, 40, 25] 2,080 Conv2d-61 [-1, 32, 40, 25] 9,248 Dropout-62 [-1, 32, 40, 25] 0 LeakyReLU-63 [-1, 32, 40, 25] 0 Conv2d-64 [-1, 32, 40, 25] 3,104 Conv2d-65 [-1, 32, 40, 25] 9,248 Dropout-66 [-1, 32, 40, 25] 0 LeakyReLU-67 [-1, 32, 40, 25] 0 BatchNorm2d-68 [-1, 32, 40, 25] 64 DenseNet2D_down_block-69 [-1, 32, 40, 25] 0 Conv2d-70 [-1, 32, 80, 50] 2,080 Conv2d-71 [-1, 32, 80, 50] 9,248 Dropout-72 [-1, 32, 80, 50] 0 LeakyReLU-73 [-1, 32, 80, 50] 0 Conv2d-74 [-1, 32, 80, 50] 3,104 Conv2d-75 [-1, 32, 80, 50] 9,248 Dropout-76 [-1, 32, 80, 50] 0 LeakyReLU-77 [-1, 32, 80, 50] 0 DenseNet2D_up_block_concat-78 [-1, 32, 80, 50] 0 Conv2d-79 [-1, 32, 160, 100] 2,080 Conv2d-80 [-1, 32, 160, 100] 9,248 Dropout-81 [-1, 32, 160, 100] 0 LeakyReLU-82 [-1, 32, 160, 100] 0 Conv2d-83 [-1, 32, 160, 100] 3,104 Conv2d-84 [-1, 32, 160, 100] 9,248 Dropout-85 [-1, 32, 160, 100] 0 LeakyReLU-86 [-1, 32, 160, 100] 0 DenseNet2D_up_block_concat-87 [-1, 32, 160, 100] 0 Conv2d-88 [-1, 32, 320, 200] 2,080 Conv2d-89 [-1, 32, 320, 200] 9,248 Dropout-90 [-1, 32, 320, 200] 0 LeakyReLU-91 [-1, 32, 320, 200] 0 Conv2d-92 [-1, 32, 320, 200] 3,104 Conv2d-93 [-1, 32, 320, 200] 9,248 Dropout-94 [-1, 32, 320, 200] 0 LeakyReLU-95 [-1, 32, 320, 200] 0 DenseNet2D_up_block_concat-96 [-1, 32, 320, 200] 0 Conv2d-97 [-1, 32, 640, 400] 2,080 Conv2d-98 [-1, 32, 640, 400] 9,248 Dropout-99 [-1, 32, 640, 400] 0 LeakyReLU-100 [-1, 32, 640, 400] 0 Conv2d-101 [-1, 32, 640, 400] 3,104 Conv2d-102 [-1, 32, 640, 400] 9,248 Dropout-103 [-1, 32, 640, 400] 0 LeakyReLU-104 [-1, 32, 640, 400] 0 DenseNet2D_up_block_concat-105 [-1, 32, 640, 400] 0 Dropout-106 [-1, 32, 640, 400] 0 Conv2d-107 [-1, 4, 640, 400] 132 ================================================================ Total params: 248,900 Trainable params: 248,900 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.98 Forward/backward pass size (MB): 1920.41 Params size (MB): 0.95 Estimated Total Size (MB): 1922.34 ----------------------------------------------------------------","title":"Contents in the zip folder"},{"location":"legacy/v1/README-visualpp/","text":"Visual Post-Processing Pipeline Part of ILLIXR , the Illinios Extended Reality Benchmark Suite. This repo contains code for lens distortion correction, chromatic aberration correction, and timewarp. Some of the FBO intialization code is borrowed from Song Ho Ahn's excellent tutorial series. The particular FBO code used is found at his website . The image loader is based on Morten Nobel-J\u00f8rgensen's blog post . Compiling and Running Compile on Linux with the included makefile. Run with ./fbo <input image> We provide three examples, landscape.png , museum.png , and tundra.png , but any PNG image should work.","title":"Visual Post-processing (Version 1)"},{"location":"legacy/v1/README-visualpp/#visual-post-processing-pipeline","text":"Part of ILLIXR , the Illinios Extended Reality Benchmark Suite. This repo contains code for lens distortion correction, chromatic aberration correction, and timewarp. Some of the FBO intialization code is borrowed from Song Ho Ahn's excellent tutorial series. The particular FBO code used is found at his website . The image loader is based on Morten Nobel-J\u00f8rgensen's blog post .","title":"Visual Post-Processing Pipeline"},{"location":"legacy/v1/README-visualpp/#compiling-and-running","text":"Compile on Linux with the included makefile. Run with ./fbo <input image> We provide three examples, landscape.png , museum.png , and tundra.png , but any PNG image should work.","title":"Compiling and Running"}]}